[{"content":"Introduction It is common that one would have multiple Github accounts, such as one for personal use and one for work. Managing Github repositories requires a developer to set up SSH keys on his/her computer. However, this becomes non-trivial when one has to work with multiple accounts representing different identities. This blog post describes how one can easily manage multiple Github repositories from different accounts on the same computer.\nScenario Let\u0026rsquo;s assume that I have a two Github accounts, one is albert-personal and one is albert-work. I use the former to work on my personal projects, and use the latter when working on repositories from my work. Let\u0026rsquo;s also assume that we have two repositories, one is personal-project and another is work-project, which are under the two accounts mentioned above respectively.\nAlso when accessing these two accounts, I need to use different SSH keys. Let\u0026rsquo;s say I have the following keys in my .ssh folder:\nalbert-personal # private key for personal account albert-personal.pub # public key for personal account albert-work # private key for work account albert-work.pub # private key for work account Our goal is to set up the repositories to use the corresponding SSH key when pushing and pulling from Github.\nSolution We want to set up the local repository to use a certain SSH key. To do so, we first create a new config file under the .ssh folder.\nFirstly, we create a file ~/.ssh/config-personal with the following content:\nHost github.com HostName github.com Port 22 User git IdentifyFile ~/.ssh/albert-personal and also a file ~/.ssh/config-work with the following content\nHost github.com HostName github.com Port 22 User git IdentifyFile ~/.ssh/albert-work These configuration files basically tells the ssh program to use a certain SSH key when accessing a certain domain (github.com in this case).\nNext we need to configure each local repository to use the corresponding SSH configuration file when pushing or pulling from Github. Every Git repository has a .git folder, in which there is a file named config that stores some configurations of the repository. It looks something like this:\n[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true [remote \u0026#34;origin\u0026#34;] url = git@github.com:username/repo-name.git fetch = +refs/heads/*:refs/remotes/origin/* [branch \u0026#34;main\u0026#34;] remote = origin merge = refs/heads/main We can add a new line into this config file to ask git to use a certain SSH configuration file. This is done by adding the following line under the [core] section in personal-project/.git/config:\nsshCommand = ssh -F ~/.ssh/config-personal and in work-project/.git/config:\nsshCommand = ssh -F ~/.ssh/config-work Finally, make sure that you have added the SSH public keys to the corresponding Github account.\n","date":"2023-08-27T00:00:00Z","permalink":"https://albertauyeung.github.io/2023/08/27/multiple-github-account.html","title":"ğŸ’» Managing multiple Github accounts on the same computer"},{"content":"\nLike most people, I first knew about Andy Weir through the movie Martian. It was a great sci-fi movie with the science presented accurately. Therefore, when I knew that Andy Weir\u0026rsquo;s latest science fiction, Project Hail Mary, was released, I immediately bought the book and started reading.\nProject Hail Mary tells the story of Ryland Grace, a high school scienc teacher, who was involved in a mission to save the Solar System. It turns out to be another great science fiction, which in my opinion won\u0026rsquo;t disappoint anyone who is attracted to it because of Andy Weir\u0026rsquo;s previous fictions. It definitely is the best book I read in 2021. I was later delighted to see that Bill Gates featured Project Hail Mary as one of the five books he loved reading this year, and the book was also the winner of in the sci-fi category in the 2021 Goodreads Choice Award.\nIn many aspect, Project Hail Mary is similar to Martian. Both protagonists are scientists who were thrown into desperate situations in which they have to fight for their own survival with their scientific knowledge. However, in Project Hail Mary, Ryland\u0026rsquo;s purpose is not only to survive but also to make sure that the mission to save the solar system and the human race. This adds more complexity into the story, as well as the decision making processes that Ryland has to gone through.\nAlthough I enjoy different types of science fiction, I always find it more engaging when the aurthor attempts to present sciecne as accurate as possible and to explain things that appear in the story with existing scientific knowledge. That is why I enjoy reading Project Hail Mary very much. I highly recommend the book to anyone who is looking for something fun and inspiring to read over a weekend.\n","date":"2021-12-20T00:00:00Z","permalink":"https://albertauyeung.github.io/2021/12/20/project-hail-mary.html","title":"ğŸ›¸ Project Hail Mary"},{"content":"æœ€è¿‘åœ¨çœ‹ Daniel Kahneman çš„æ–°æ›¸ Noiseï¼ˆä¸­è­¯ã€Šé›œè¨Šã€‹ï¼‰ã€‚Daniel Kahneman æ˜¯2002å¹´è«¾è²çˆ¾ç¶“æ¿Ÿå­¸å¥¬å¾—ä¸»ï¼Œä»–çš„å‰ä½œ Thinking Fast and Slowï¼ˆä¸­è­¯ã€Šå¿«æ€æ…¢æƒ³ã€‹ï¼‰åŸºæ–¼ä»–è·Ÿæ‹æª” Amos Tversky å¤šå¹´ä¾†çš„ç ”ç©¶ï¼Œä»‹ç´¹äººé¡çš„å…©å¤§æ€è€ƒæ¨¡å¼ï¼Œå¼•èµ·å¾ˆå¤§è¿´éŸ¿ã€‚å¦‚æœæ²¡æœ‰çœ‹éè©²æ›¸ï¼Œå¯ä»¥å…ˆçœ‹çœ‹ Kahneman 2011å¹´åœ¨ Google çš„æ¼”è¬›ã€‚é€™æ¬¡çš„æ–°æ›¸ï¼Œå‰‡ä¸»è¦ä»‹ç´¹åŠèªªæ˜æ±ºç­–éç¨®ç”±æ–¼å„ç¨®åŸå› å‡ºç¾çš„é›œè¨Šï¼Œå°è‡´äººå€‘åœ¨é¢å°ç›¸åŒçš„å•é¡Œæ™‚ä¹Ÿå¯ä»¥çµ¦å‡ºå®Œå…¨ä¸ä¸€æ¨£çš„åˆ¤æ–·ã€‚ä»–æŒ‡å‡ºï¼Œé€™ç¨®åœ¨æ±ºç­–ä¸­ç¾çš„é›œè¨Šéš¨è™•å¯è¦‹ï¼Œæ›¸ä¸­çš„ä¾‹å­æ›´æ˜¯ä»¤äººå¤§é–‹çœ¼ç•Œã€‚\nåœ¨ç¾åœ‹æœ‰äººæ›¾ç¶“ç ”ç©¶éæ³•å®˜çš„åˆ¤åˆ‘æ±ºå®šï¼Œä¸åŒçš„æ³•å®˜å°æŸä¸€æ¡ˆä»¶çš„åˆ¤åˆ‘å¯ä»¥æ˜¯ä¸€å¹´åˆ°åäº”å¹´ï¼Œæœ‰äº›ç ”ç©¶æ›´æŒ‡å‡ºæ³•å®˜çš„åˆ¤æ±ºè·Ÿæœƒå—ä¸€äº›çœ‹ä¾†æ¯«ä¸ç›¸é—œçš„å› ç´ å½±éŸ¿ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ¬åœ°çš„è¶³çƒéšŠåœ¨é€±æœ«è¼¸äº†ä¸€å ´æ¯”è³½ï¼Œæ³•å®˜å€‘åœ¨æ¥ä¸‹ä¾†çš„æ˜ŸæœŸä¸€æ‰€ä½œå‡ºçš„åˆ¤æ±ºæœƒæ¯”è¼ƒé‡ã€‚åœ¨æ³•åœ‹ï¼Œå¦‚æœåˆ¤æ±ºç•¶å¤©æ˜¯è¾¯è­·äººçš„ç”Ÿæ—¥ï¼Œæ³•å®˜ä½œå‡ºçš„åˆ¤æ±ºæœƒè¼ƒè¼•ã€‚æ›¸ä¸­å¦ä¸€å€‹ä¾‹å­ç™¼ç”Ÿåœ¨ Kahneman è‡ªå·±æ¥è§¸éçš„ä¸€å®¶ä¿éšªå…¬å¸ã€‚ä»–ç™¼ç¾ä¿éšªå…¬å¸ä¸åŒå“¡å·¥åœ¨ä¼°ç®—åŒä¸€ä¿å–®çš„åƒ¹æ ¼æˆ–è³ å„Ÿé‡‘é¡æ™‚æ‰€ä½œçš„æ±ºå®šä¸¦ä¸ä¸€è‡´ï¼Œå·®é¡æ›´å¯ä»¥è¶…é 50%ï¼Œè€Œå…¬å¸çš„ç®¡ç†å±¤å»ä¸€ç›´èªç‚ºä¸åŒå“¡å·¥æ‰€ä½œçš„æ±ºå®šç›¸å·®ä¸æœƒè¶…é 10%ã€‚\næ›´æœ‰è¶£çš„ä¸€é …ç ”ç©¶è¦‹æ–¼ä¸€ä½ä¾†è‡ªè¥¿ç­ç‰™çš„è¡Œç‚ºå¿ƒç†å­¸æ•™æˆ Uri Simonsohn ç™¼è¡¨çš„ä¸€ç¯‡è«–æ–‡ï¼Œé¡Œç‚º \u0026ldquo;Clouds Make Nerds Look Good\u0026quot;ã€‚ä»–ç™¼ç¾å¤©æ°£å°å¤§å­¸å…¥å­¸ç”³è«‹çš„å¯©æ‰¹å±…ç„¶æœ‰æ˜é¡¯çš„å½±éŸ¿ï¼š å¤©æ°£ä¸å¥½çš„æ™‚å€™ï¼Œè² è²¬å¯©æ‰¹ç”³è«‹çš„äººå“¡æœƒæ›´çœ‹é‡å­¸ç”Ÿçš„å­¸è¡“æˆç¸¾ï¼Œè€Œåœ¨å¤©æ°£æ™´æœ—çš„æ—¥å­ï¼Œåœ¨å…¶ä»–éå­¸è¡“æ–¹é¢è¡¨ç¾çªå‡ºçš„å­¸ç”Ÿæœƒæ¯”è¼ƒæœ‰åˆ©ï¼\nKahneman åœ¨æ›¸ä¸­æŒ‡å‡ºï¼Œæ±ºç­–çš„éŒ¯èª¤ä¾†è‡ªå…©å€‹å› ç´ ï¼Œä¸€ç‚ºã€Œåè¦‹ã€(bias)ï¼ŒäºŒç‚ºã€Œé›œè¨Šã€(noise)ï¼ˆæ›´ç‚ºæ­£ç¢ºçš„åç¨±æ‡‰è©²æ˜¯ varianceï¼Œå³åˆ¤æ–·åé›¢å¹³å‡å€¼çš„ç¨‹åº¦ï¼‰ã€‚ä¸€èˆ¬æƒ…æ³ä¸‹ï¼Œäººå€‘æ›´å®¹æ˜“å¯Ÿè¦ºåˆ°åè¦‹çš„å­˜åœ¨ï¼Œè€Œå¿½ç•¥é›œè¨Šï¼Œä½†ä¿®æ­£åè¦‹è·Ÿæ¸›ä½é›œè¨Šï¼Œå°é¿å…ä½œå‡ºéŒ¯èª¤åˆ¤æ–·çš„è²¢ç»æ˜¯ä¸€æ¨£çš„ã€‚æ•…æ­¤ä»–èªç‚ºæˆ‘å€‘ä¸æ‡‰å°æ±ºç­–éç¨‹ä¸­å‡ºç¾çš„é›œè¨Šè¦–è€Œä¸è¦‹ã€‚\nä½œç‚ºä¸€å€‹åœ¨å·¥ä½œä¸­ç¶“å¸¸æ‡‰ç”¨åˆ°å„ç¨®æ©Ÿå™¨å­¸ç¿’æŠ€è¡“çš„è»Ÿä»¶å·¥ç¨‹å¸«ï¼Œé€™äº›æ¦‚å¿µä¸€é»éƒ½ä¸é™Œç”Ÿã€‚å…¶å¯¦ï¼Œbias å’Œ variance æ­£æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­éå¸¸é‡è¦çš„æ¦‚å¿µï¼Œä¸»è¦ç”¨æ–¼åˆ†æä¸€å€‹æ¨¡å‹æ˜¯å¦æº–ç¢ºæè¿°æ•¸æ“šä¹‹é–“çš„é—œä¿‚ã€‚\nä¸€å€‹æ¨¡å‹ bias æ¯”è¼ƒå¤§æ™‚ï¼Œèªªæ˜å®ƒéä»½ç°¡å–®ï¼Œæœªèƒ½å®Œå…¨åæ˜ ç¾å¯¦ä¸–ç•Œä¸­äº‹ç‰©ä¹‹é–“çš„è¤‡é›œé—œä¿‚ã€‚ä¾‹å¦‚ï¼Œç•¶è¦é æ¸¬ä¸€å€‹äººæ˜¯å¦æœ‰å¿ƒè‡Ÿç—…æ™‚ï¼Œå¦‚æœæ¨¡å‹åªèªç‚ºå¹´é½¡è¶Šå¤§ï¼Œå‰‡æœ‰å¿ƒè‡Ÿç—…çš„æ©Ÿç‡è¶Šå¤§ï¼Œå¾ˆæ˜é¡¯é€™å€‹æ¨¡å‹ä¸æœƒååˆ†æº–ç¢ºã€‚æ›å¥è©±èªªï¼Œé€™å€‹æ¨¡å‹çš„ã€Œåè¦‹ã€ååˆ†åš´é‡ï¼ˆéä»½ç°¡å–®ï¼‰ï¼Œå› ç‚ºå®ƒåªåƒè€ƒä¸€å€‹äººçš„å…¶ä¸­ä¸€å€‹ç‰¹å¾´ä¾†ä½œå‡ºåˆ¤æ–·ï¼Œè€Œä¸”åªè€ƒæ…®æ­¤ç‰¹å¾µè·Ÿçµæœçš„ç·šæ€§é—œä¿‚ã€‚ å¦ä¸€æ–¹é¢ï¼Œä¸€å€‹æ¨¡å‹çš„ variance æ¯”è¼ƒå¤§æ™‚ï¼Œèªªæ˜å®ƒéä»½è¤‡é›œï¼Œåœ¨å­¸ç¿’æ•¸æ“šä¹‹é–“çš„é—œä¿‚æ™‚ï¼ŒæœƒæŠŠä¸€äº›æœ¬ä¾†è·Ÿçµæœæ²¡æœ‰é—œä¿‚ã€åªæ˜¯å¶çˆ¾å‡ºç¾åœ¨å€‹åˆ¥æƒ…æ³çš„ç‰¹å¾µä¹Ÿç´å…¥è€ƒæ…®ï¼Œå°è‡´æ¨¡å‹åœ¨å°‡ä¾†çš„æ‡‰ç”¨ä¸­æœªèƒ½ä½œå‡ºæº–ç¢ºçš„åˆ¤æ–·ï¼ˆå› ç‚ºå®ƒå¾ˆå®¹æ˜“è¢«æ•¸æ“šä¸­çš„é›œè¨Šèª¤å°ï¼‰ï¼Œä¹Ÿå°±æ˜¯åœ¨è¨“ç·´æ¨¡å‹æ™‚ç¶“å¸¸æœƒæåŠçš„æ‰€è¬‚ overfittingï¼ˆéåº¦æ“¬åˆï¼‰ã€‚ ä¸€èˆ¬äººå¯èƒ½èªç‚ºåœ¨ä½œå‡ºå„ç¨®å°ˆæ¥­åˆ¤æ–·çš„æ™‚å€™ï¼Œå°ˆæ¥­çŸ¥è­˜æ˜¯æœ€é‡è¦çš„å› ç´ ï¼Œä½† Kahneman åœ¨é€™æœ¬æ›¸è£¡é¢æ˜ç¢ºæŒ‡å‡ºæ±ºç­–éç¨‹ä¸­çš„é›œè¨Šè·Ÿå°ˆæ¥­çŸ¥è­˜æ˜¯å¦å…¨é¢æ²¡æœ‰å¿…ç„¶é—œä¿‚ã€‚è¦é¿å…éŒ¯èª¤çš„åˆ¤æ–·ï¼Œé™¤äº†è¦æ’é™¤ã€Œåè¦‹ã€/æ“æœ‰å…¨é¢çš„çŸ¥è­˜å¤–ï¼Œæ›´è¦é—œæ³¨ä½œå‡ºåˆ¤æ–·æ™‚çš„æ‰€å‡ºç¾çš„é›œè¨Šã€‚\n","date":"2021-11-08T00:00:00Z","permalink":"https://albertauyeung.github.io/2021/11/08/noise-daniel-kahneman.html","title":"ğŸ”Š Noise é›œè¨Š"},{"content":"It is common that the different projects you are working on depend on different versions of Python. That is why pyenv becomes very handy for Python developers, as it lets you switch between different Python versions easily. With pyenv-virtualenv it can also be used together with virtualenv to create isolated development environments for different projects with different dependencies.\nFor example, if some of the projects you are working on requires Tensorflow 1.15, while your system\u0026rsquo;s Python is of version 3.8, you must find some ways to install Python 3.7 in order to work on your project, as Tensorflow 1.15 can only be run in Python 3.5 to Python 3.7.\nThis article aims at giving a quick introduction to pyenv and pyenv-virtualenv, as well as describing how one can easily create new kernels of virtual environments in Jupyter.\nInstalling and Using pyenv pyenv works on macOS and Linux, but not Windows (except inside the Windows Subsystem for Linux). Windows users might want to check out pyenv-win for further information.\nOn macOS, it can be installed using Homebrew:\n$ brew update $ brew install pyenv On both macOS and Linux, it can also be installed by checking out the latest version of pyenv. For details of installing pyenv this way, refer to the offical installation guidelines here: https://github.com/pyenv/pyenv#installation.\nAfter installation, add the following line to your .bashrc (or .zshrc) file:\neval \u0026#34;$(pyenv init -)\u0026#34; Once you have pyenv installed, you can do a few things like below:\nInstalling a Python version\n# List all available Python versions $ pyenv install --list # Install a specific Python version (3.7.8) $ pyenv install 3.7.8 # List Python version installed $ pyenv versions * system (set by /Users/....) 3.7.8 Setting a local Python version\n# Set the Python version for the current directory $ pyenv local 3.7.8 # Now by default you will be using Python 3.7.8 $ python Python 3.7.8 (default, Aug 17 2020, 11:05:21) \u0026gt;\u0026gt;\u0026gt; # Unset it and change back to system default $ pyenv local --unset Setting a global Python version\n# Install a new version and set it as system default $ pyenv install 2.7.6 $ pyenv global 2.7.6 # Now you have 2.7.6 as the default Python version $ python Python 2.7.6 (default, Aug 17 2020, 11:08:23) \u0026gt;\u0026gt;\u0026gt; Using virtualenv with pyenv pyenv by itself only allows you to switch between different Python versions. To create an isolated environment with a set of dependencies, we will need virtualenv too. You can follow the steps below to set up your computer to use pyenv and virtualenv together.\nFirstly, we need ot install virtualenv:\n$ pip3 install virtualenv $ pip3 install virtualenvwrapper Next, we need to install pyenv-virtualenv. This can be done on macOS by using brew as follows (or follow the instructions on this page if you are not using macOS):\n$ brew install pyenv-virtualenv Finally, add the following line to your .bashrc or .zshrc file:\neval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; Once you are done with the steps above, you can create new virtual environments as follows:\n# Install a new Python version $ pyenv install 3.7.4 # Create a new virtualenv named myenv with Python 3.7.4 $ pyenv virtualenv 3.7.4 tf1.15 # Go to the project directory, and set its local environment $ cd ~/repo/my-project $ pyenv local tf1.15 # Install dependencies as needed $ pip3 install tensorflow==1.15 Adding Kernels to Jupyter It is also common that we use Jupyter for quick prototyping and testing. It would be convenient if we can invoke different virtual environments in Jupyter to test our source codes. In fact, it is very easy to create new kernels of different virtual environments in Jupyter.\nFirstly, you have to check the paths of your Juypyter installation. (Note that it does not matter which environment you are using to run your Jupyter notebook or Jupyter lab.) You can check the paths using the following command:\n$ jupyter --paths On my computer, it is something like below. What we need to note here is the data path.\nconfig: /Users/albert/.jupyter /usr/local/Cellar/python@3.8/3.8.4/Frameworks/Python.framework/Versions/3.8/etc/jupyter /usr/local/etc/jupyter /etc/jupyter data: /Users/albert/Library/Jupyter /usr/local/Cellar/python@3.8/3.8.4/Frameworks/Python.framework/Versions/3.8/share/jupyter /usr/local/share/jupyter /usr/share/jupyter runtime: /Users/ayeung/Library/Jupyter/runtime Next, we will need to check the path to the Python interpreter of the virtual environment:\n# Activate your virtualenv $ pyenv activate tf1.15 # Check path of the Python interpreter $ pyenv which python /Users/albert/.pyenv/versions/tf1.15/bin/python # Deactivate the virtualenv $ pyenv deactivate Finally, we create a new folder under the kernels directory:\n$ mkdir /User/albert/Library/Jupyter/kernels/tf1.15 and add a new file named kernel.json in that directory with the following content:\n{ \u0026#34;argv\u0026#34;: [ \u0026#34;/User/albert/.pyenv/versions/tf1.15/bin/python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;ipykernel\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;{connection_file}\u0026#34; ], \u0026#34;display_name\u0026#34;: \u0026#34;tf1.15\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34; } Once this is done, you will be able to use the kernel in Jupyter.\n","date":"2020-08-17T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/08/17/pyenv-jupyter.html","title":"ğŸª pyenv, virtualenv and using them with Jupyter"},{"content":"To use a pre-trained BERT model, we need to convert the input data into an appropriate format so that each sentence can be sent to the pre-trained model to obtain the corresponding embedding. This article introduces how this can be done using modules and functions available in Hugging Face\u0026rsquo;s transformers package (https://huggingface.co/transformers/index.html).\nInput Representation in BERT Let\u0026rsquo;s first try to understand how an input sentence should be represented in BERT. BERT embeddings are trained with two training tasks:\nClassification Task: to determine which category the input sentence should fall into Next Sentence Prediction Task: to determine if the second sentence naturally follows the first sentence. The [CLS] and [SEP] Tokens For the classification task, a single vector representing the whole input sentence is needed to be fed to a classifier. In BERT, the decision is that the hidden state of the first token is taken to represent the whole sentence. To achieve this, an additional token has to be added manually to the input sentence. In the original implementation, the token [CLS] is chosen for this purpose.\nIn the \u0026ldquo;next sentence prediction\u0026rdquo; task, we need a way to inform the model where does the first sentence end, and where does the second sentence begin. Hence, another artificial token, [SEP], is introduced. If we are trying to train a classifier, each input sample will contain only one sentence (or a single text input). In that case, the [SEP] token will be added to the end of the input text.\nIn summary, to preprocess the input text data, the first thing we will have to do is to add the [CLS] token at the beginning, and the [SEP] token at the end of each input text.\nPadding Token [PAD] The BERT model receives a fixed length of sentence as input. Usually the maximum length of a sentence depends on the data we are working on. For sentences that are shorter than this maximum length, we will have to add paddings (empty tokens) to the sentences to make up the length. In the original implementation, the token [PAD] is used to represent paddings to the sentence.\nConverting Tokens to IDs When the BERT model was trained, each token was given a unique ID. Hence, when we want to use a pre-trained BERT model, we will first need to convert each token in the input sentence into its corresponding unique IDs.\nThere is an important point to note when we use a pre-trained model. Since the model is pre-trained on a certain corpus, the vocabulary was also fixed. In other words, when we apply a pre-trained model to some other data, it is possible that some tokens in the new data might not appear in the fixed vocabulary of the pre-trained model. This is commonly known as the out-of-vocabulary (OOV) problem.\nFor tokens not appearing in the original vocabulary, it is designed that they should be replaced with a special token [UNK], which stands for unknown token.\nHowever, converting all unseen tokens into [UNK] will take away a lot of information from the input data. Hence, BERT makes use of a WordPiece algorithm that breaks a word into several subwords, such that commonly seen subwords can also be represented by the model.\nFor example, the word characteristically does not appear in the original vocabulary. Nevertheless, when we use the BERT tokenizer to tokenize a sentence containing this word, we get something as shown below:\n\u0026gt;\u0026gt;\u0026gt; from transformers import BertTokenizer \u0026gt;\u0026gt;\u0026gt; tz = BertTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) \u0026gt;\u0026gt;\u0026gt; tz.convert_tokens_to_ids([\u0026#34;characteristically\u0026#34;]) [100] \u0026gt;\u0026gt;\u0026gt; sent = \u0026#34;He remains characteristically confident and optimistic.\u0026#34; \u0026gt;\u0026gt;\u0026gt; tz.tokenize(sent) [\u0026#39;He\u0026#39;, \u0026#39;remains\u0026#39;, \u0026#39;characteristic\u0026#39;, \u0026#39;##ally\u0026#39;, \u0026#39;confident\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;optimistic\u0026#39;, \u0026#39;.\u0026#39;] \u0026gt;\u0026gt;\u0026gt; tz.convert_tokens_to_ids(tz.tokenize(sent)) [1124, 2606, 7987, 2716, 9588, 1105, 24876, 119] We can see that the word characteristically will be converted to the ID 100, which is the ID of the token [UNK], if we do not apply the tokenization function of the BERT model.\nThe BERT tokenization function, on the other hand, will first breaks the word into two subwoards, namely characteristic and ##ally, where the first token is a more commonly-seen word (prefix) in a corpus, and the second token is prefixed by two hashes ## to indicate that it is a suffix following some other subwords.\nAfter this tokenization step, all tokens can be converted into their corresponding IDs.\nSummary In summary, an input sentence for a classification task will go through the following steps before being fed into the BERT model.\nTokenization: breaking down of the sentence into tokens Adding the [CLS] token at the beginning of the sentence Adding the [SEP] token at the end of the sentence Padding the sentence with [PAD] tokens so that the total length equals to the maximum length Converting each token into their corresponding IDs in the model An example of preparing a sentence for input to the BERT model is shown below. For simplicity, we assume the maximum length is 10 in the example below (while in the original model it is set to be 512).\n# Original Sentence Let\u0026#39;s learn deep learning! # Tokenized Sentence [\u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;] # Adding [CLS] and [SEP] Tokens [\u0026#39;[CLS]\u0026#39;, \u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;[SEP]\u0026#39;] # Padding [\u0026#39;[CLS]\u0026#39;, \u0026#39;Let\u0026#39;, \u0026#34;\u0026#39;\u0026#34;, \u0026#39;s\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;deep\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;!\u0026#39;, \u0026#39;[SEP]\u0026#39;, \u0026#39;[PAD]\u0026#39;] # Converting to IDs [101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0] Tokenization using the transformers Package While there are quite a number of steps to transform an input sentence into the appropriate representation, we can use the functions provided by the transformers package to help us perform the tokenization and transformation easily. In particular, we can use the function encode_plus, which does the following in one go:\nTokenize the input sentence Add the [CLS] and [SEP] tokens. Pad or truncate the sentence to the maximum length allowed Encode the tokens into their corresponding IDs Pad or truncate all sentences to the same length. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens The following codes shows how this can be done.\n# Import tokenizer from transformers package from transformers import BertTokenizer # Load the tokenizer of the \u0026#34;bert-base-cased\u0026#34; pretrained model # See https://huggingface.co/transformers/pretrained_models.html for other models tz = BertTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) # The senetence to be encoded sent = \u0026#34;Let\u0026#39;s learn deep learning!\u0026#34; # Encode the sentence encoded = tz.encode_plus( text=sent, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length = 64, # maximum length of a sentence pad_to_max_length=True, # Add [PAD]s return_attention_mask = True, # Generate the attention mask return_tensors = \u0026#39;pt\u0026#39;, # ask the function to return PyTorch tensors ) # Get the input IDs and attention mask in tensor format input_ids = encoded[\u0026#39;input_ids\u0026#39;] attn_mask = encoded[\u0026#39;attention_mask\u0026#39;] After executing the codes above, we will have the following content for the input_ids and attn_mask variables:\n\u0026gt;\u0026gt;\u0026gt; input-ids tensor([[ 101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; attn_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) The \u0026ldquo;attention mask\u0026rdquo; tells the model which tokens should be attended to and which (the [PAD] tokens) should not (see the documentation for more detail). It will be needed when we feed the input into the BERT model.\nReference Devlin et al. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding BERT - transformers documentation: https://huggingface.co/transformers/model_doc/bert.html ","date":"2020-06-19T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/06/19/bert-tokenization.html","title":"ğŸ¤– Mastering BERT Tokenization and Encoding"},{"content":"What is a Trie? Trie is a very useful data structure. It is commonly used to represent a dictionary for looking up words in a vocabulary.\nFor example, consider the task of implementing a search bar with auto-completion or query suggestion. When the user enters a query, the search bar will automatically suggests common queries starting with the characters input by the user.\nTo implement such a function, we need several things at the backend. The first, obviously, is a list of common queries. Or it can be a list of proper English words for the purpose of auto-completion). Secondly, we will need to have an algorithm to quickly look up words starting with the characters input by the user, and this is where we need to use the trie data structure.\nThe follow example illustrates why a special data structure is necessary to look up words quickly given a prefix:\nThe user inputs the characters en In our dictionary, we have the following words starting with en: english, entertainment Commonly used data structures such as list and dictionary in Python do not allow quick look up of elements stored inside. For example, to see if there is any word having the prefix en in a Python dictionary, we cannot avoid going through each of the keys, resulting in O(n) time, where n is the number of entries in the dictionary Trie is a tree-like data structure made up of nodes. Nodes can be used to store data. Each node may have none, one or more children. When used to store a vocabulary, each node is used to store a character, and consequently each \u0026ldquo;branch\u0026rdquo; of the trie represents a unique word. The following figure shows a trie with five words (was, wax, what, word, work) stored in it.\nHow does a Trie Work? There are two major operations that can be performed on a trie, namely:\nInserting a word into the trie Searching for words using a prefix Both operations involves traversing the trie by starting from the root node. We take a look at each of these operations in more detail.\nInserting Words into the Trie In order to insert a new word into the trie, we need to first check whether any prefix of the word is already in the trie. Therefore, we will start traverse the trie from the root node, and follow the algorithm below:\nSet the current node to be the root node Set the current character as the first character of the input word Check if the current character is a child of the current node If yes, set the current node to be this child node, set the current character to the next character in the input word, and perform this step again If no, it means from this character onwards, we will need to create new nodes and insert them into the trie Below is an illustration of what will happen when we want to add the word won into the trie above.\nFollowing the steps in the algorithm mentioned above, we will arrive at the node o under w, at which point we discover that n is not a child of o, and therefore we create a new node for the character n, and insert it under o.\nSearching in the Trie A common application scenario of the trie data structure is to search for words with a certain prefix, just like the auto-complete or query suggestion function in a search bar.\nWhen given a prefix, we can traverse the trie to check if any word in the trie starts with that prefix. If the prefix is found in the trie, we can then use depth-first traversal to retrieve all the words with that prefix.\nFor example, given the trie illustrated above, which contains the words was, wax, what, word, work and won, let\u0026rsquo;s see what will happen if we want to search for words with the prefix wa:\nStarting from the root node, we are able to find the node w and a From the node a, we can go on to traverse the trie to retrieve all words starting with the prefix wa When we arrive at the node s, we check whether it is the end of a word (yes), and the word was was output Similarity, when we arrive at the node x, the word wax is output Implementing Trie in Python To implement a trie, we can first create a TrieNode class, which can be used to represent a node in the trie. Below is how this class can be implemented.\nclass TrieNode: \u0026#34;\u0026#34;\u0026#34;A node in the trie structure\u0026#34;\u0026#34;\u0026#34; def __init__(self, char): # the character stored in this node self.char = char # whether this can be the end of a word self.is_end = False # a counter indicating how many times a word is inserted # (if this node\u0026#39;s is_end is True) self.counter = 0 # a dictionary of child nodes # keys are characters, values are nodes self.children = {} In this implementation, we want to store also the number of times a word has been inserted into the trie. This allows us to support additional features, such as ranking the words by their popularity.\nGiven the TrieNode class, we can go on to implement the Trie class as follows.\nclass Trie(object): \u0026#34;\u0026#34;\u0026#34;The trie object\u0026#34;\u0026#34;\u0026#34; def __init__(self): \u0026#34;\u0026#34;\u0026#34; The trie has at least the root node. The root node does not store any character \u0026#34;\u0026#34;\u0026#34; self.root = TrieNode(\u0026#34;\u0026#34;) def insert(self, word): \u0026#34;\u0026#34;\u0026#34;Insert a word into the trie\u0026#34;\u0026#34;\u0026#34; node = self.root # Loop through each character in the word # Check if there is no child containing the character, create a new child for the current node for char in word: if char in node.children: node = node.children[char] else: # If a character is not found, # create a new node in the trie new_node = TrieNode(char) node.children[char] = new_node node = new_node # Mark the end of a word node.is_end = True # Increment the counter to indicate that we see this word once more node.counter += 1 def dfs(self, node, prefix): \u0026#34;\u0026#34;\u0026#34;Depth-first traversal of the trie Args: - node: the node to start with - prefix: the current prefix, for tracing a word while traversing the trie \u0026#34;\u0026#34;\u0026#34; if node.is_end: self.output.append((prefix + node.char, node.counter)) for child in node.children.values(): self.dfs(child, prefix + node.char) def query(self, x): \u0026#34;\u0026#34;\u0026#34;Given an input (a prefix), retrieve all words stored in the trie with that prefix, sort the words by the number of times they have been inserted \u0026#34;\u0026#34;\u0026#34; # Use a variable within the class to keep all possible outputs # As there can be more than one word with such prefix self.output = [] node = self.root # Check if the prefix is in the trie for char in x: if char in node.children: node = node.children[char] else: # cannot found the prefix, return empty list return [] # Traverse the trie to get all candidates self.dfs(node, x[:-1]) # Sort the results in reverse order and return return sorted(self.output, key=lambda x: x[1], reverse=True) Below is an example of how this Trie class can be used:\n\u0026gt;\u0026gt;\u0026gt; t = Trie() \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;was\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;word\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;war\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;what\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.insert(\u0026#34;where\u0026#34;) \u0026gt;\u0026gt;\u0026gt; t.query(\u0026#34;wh\u0026#34;) [(\u0026#39;what\u0026#39;, 1), (\u0026#39;where\u0026#39;, 1)] References Trie, Wikipedia Tries, Brilliant.org Trie (Prefix Tree) - Visualizing the operations on a trie ","date":"2020-06-15T00:00:00Z","image":"https://albertauyeung.github.io/images/query_suggestion.png","permalink":"https://albertauyeung.github.io/2020/06/15/python-trie.html","title":"ğŸŒŸ Implementing Trie in Python"},{"content":"It has been quite a long time since I have developed a Website. The last one I made was probably the Website that I designed and developed for my brother (http://www.simusic.hk/) many years ago. At that time, the Website was developed using PHP, JQuery and Bootstrap. Since then, I have been mainly working in the areas of machine learning, data mining and natural language processing, and have left frontend development behind.\nFrontend technologies, like many other areas in software development, have changed a lot in the past years. I know about Angular, React and Vue.js, but have never really gone back to learn the basics until now. Recently, I would like to explore the idea of quickly building a Website that shows dynamic content stored in a database. After some exploration, I found that the following combination seems a quick and convenient way to achieve something similar:\nUse Vue.js to build the frontend Web app Use Google Sheets to store the data Publish the Google Spreadsheet online, and retrieve the data in JSON format in the Web app This post describes how the above idea is used to implement a Web app that displays quotations stored in a Google Spreadsheet. The source code can be found on the github repository, and the Webapp can be found published here https://albertauyeung.github.io/quotes-app/. A screenshot is shown below:\nA Web App for Browsing and Searching Quotes In this small project, I developed a Web app for browsing and search quotes that are stored on a Google Spreadsheet.\nFor simplicity, this app does not allow modifying the data in the spreadsheet via the Web UI. Data has to be manipulated on the Google Sheets directly. However, this allows us to skip using the Google Sheets API in this project.\nGoogle Spreadsheet for Data Storage A google spreadsheet has been created to store the data, which can be found here. For the purpose of my project, I have a table with the following columns:\nquote: the quotation author: the person who wrote or said the text source: the source of the quotation, e.g. the name of a book tags: tags separated by commas insert_date: date on which the quotation was inserted into this sheet (press ctrl+; on Windows or Cmd+; on MacOS to quickly insert the date of today in a Google spreadsheet) In order for the spreadsheet\u0026rsquo;s data to be avaible to other apps without the need to perform authentication, we need to first publish it. This can be done by choosing Publish to the Web from the File menu in Google Sheets.\nTo access the data stored in the spreadsheet, we can send a HTTP GET request to the following URL:\nhttps://spreadsheets.google.com/feeds/list/{sheet_id}/1/public/values?alt=json You will have to replace {sheet_id} with the ID of the Google spreadsheet in which data is stored.\nSending a request to the above URL will result in a JSON response in the following format:\n{ \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;, \u0026#34;feed\u0026#34;: { \u0026#34;xmlns\u0026#34;: \u0026#34;http://www.w3.org/2005/Atom\u0026#34;, ... \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;$t\u0026#34;: \u0026#34;Sheet1\u0026#34; }, \u0026#34;link\u0026#34;: [...], ... \u0026#34;entry\u0026#34;: [ { \u0026#34;id\u0026#34;: { ... }, ... \u0026#34;gsx$quote\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Wisest is she who knows she does not know.\u0026#34; }, \u0026#34;gsx$author\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Jostein Gaarder\u0026#34; }, \u0026#34;gsx$source\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;Sophie\u0026#39;s World\u0026#34; }, \u0026#34;gsx$tags\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;philosophy\u0026#34; }, \u0026#34;gsx$insertdate\u0026#34;: { \u0026#34;$t\u0026#34;: \u0026#34;22/04/2020\u0026#34; } }, ... ] } } We can see that the data can be found under data.feed.entry. Values in each cell of different columns can be retrieved by using a key in the format of gsx${column_name}, such as gsx$quote.\nVue.js for Frontend Development Vue.js is a JavaScript framework for building Web-based user interfaces and applications. In this project, it was used to develop the Web page that displays the quotations stored in the Google Sheet.\nFirstly, follow the Vue.js official guide to include the necessary JavaScript file in the HTML source code:\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vue\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; In the Web page, we have an element representing the whole application:\n\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; The following script is that used to load the content from the Google Sheet once the page is loaded:\nvar gsheet_url = \u0026#34;{{url_to_the_google_sheet_json}}\u0026#34; var app = new Vue({ el: \u0026#39;#app\u0026#39;, data () { return { info: null } }, mounted () { axios .get(gsheet_url) .then(response =\u0026gt; ( parseData(response.data.feed.entry) )) } }); The parseData function will take the list of entries in the JSON data, and store the quotations in a local data structure for display:\n// Variables to hold the parsed data var quoteList = []; var authorList = []; var tagList = []; function parseData(entries) { var authorSet = new Set(); var tagSet = new Set(); entries.forEach(function(value) { var entry = { \u0026#34;quote\u0026#34;: value.gsx$quote.$t, \u0026#34;author\u0026#34;: value.gsx$author.$t, \u0026#34;source\u0026#34;: value.gsx$source.$t, \u0026#34;tags\u0026#34;: value.gsx$tags.$t.split(\u0026#34;,\u0026#34;) }; // Add to the set of authors authorSet.add(entry.author); // Add to the set of tags entry.tags.forEach(function(t) { tagSet.add(t); }); // Push entry into the list of quotes quoteList.push(entry); }); authorList = Array.from(authorSet); authorList.sort(); tagList = Array.from(tagSet); tagList.sort(); } Finally, we can display the data using the template syntax provided by Vue.js. For example, the list of quotations are displayed using the following template:\n\u0026lt;div id=\u0026#34;quotes\u0026#34;\u0026gt; \u0026lt;div v-for=\u0026#34;quote in quotes\u0026#34; class=\u0026#34;quote\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;quote-text\u0026#34;\u0026gt;\u0026#34;{{ quote.quote }}\u0026#34;\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;span class=\u0026#34;author\u0026#34;\u0026gt;- {{ quote.author}}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;source\u0026#34; v-if=\u0026#34;quote.source != \u0026#39;\u0026#39;\u0026#34;\u0026gt;({{ quote.source }})\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;tag-container\u0026#34;\u0026gt; \u0026lt;span v-for=\u0026#34;tag in quote.tags\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt; {{ tag }} \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; var quotes = new Vue({ el: \u0026#39;#quotes\u0026#39;, data: { quotes: quoteList } }); In this project, I also tried to implement some simple filters on the Web app to allow user to quickly filter quotations by the author names or the tags assigned to them. For details, you can refer to the source code on github.\nOverall, I found this combination very useful when we want to quickly build a user interface for browsing structured data stored in a spreadsheet. This avoids the need to create a database and a backend application. I can see a lot of potentials in using this method to quickly create simple and interesting applications.\n","date":"2020-04-26T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/04/26/vuejs-google-sheets.html","title":"ğŸ“Š Build Your Dynamic Website Easily with Vue.js and Google Sheets"},{"content":"Matplotlib by default does not support displaying Unicode characters such as Chinese, Japanese and Korean characters. This post introduces two different methods to allow these characters to be shown in the graphs.\nThe issue here is that we need to configure Matplotlib to use fonts that support the characters that we want to display. To configure the font used by Matplotlib, there are two ways.\nSpecifying the Path to the Font File If you have a font file that support displaying CJK characters, you can directly provide the path to the font file using the FontProperties class in Matplotlib. This font file can be a .ttf file (TrueType Font) or a .otf file (OpenType Font). For example, you can download a OTF font that supports displaying CJK characters from Google Fonts.\nOnce we have the font file, we can create a FontProperties instance as follows:\nimport matplotlib.font_manager as fm fprop = fm.FontProperties(fname=\u0026#39;NotoSansCJKtc-Regular.otf\u0026#39;) When plotting a graph, we can provide this FonProperties instance as an argument to functions that control what words are displayed in the graph. The example below shows how to set the font for the title and the labels on the X-axis.\nimport matplotlib.pyplot as plt # Prepare some data x = list(range(20)) xticks = [\u0026#34;é¡åˆ¥{:d}\u0026#34;.format(i) for i in x] y = [random.randint(10,99) for i in x] # Plot the graph plt.figure(figsize=(8, 2)) plt.bar(x, y) plt.xticks(x, xticks, fontproperties=fprop, fontsize=12, rotation=45) plt.title(\u0026#34;åœ–1\u0026#34;, fontproperties=fprop, fontsize=18) plt.show() The effect will be as follows:\nUsing Fonts in the Font Folder Another way of using a custom font is to install the font into Matplotlib\u0026rsquo;s font folder, and update the font manager.\nFirstly, we need to know the path to the font folder. We can first use the following command to check the location of the Matplotlib installation:\nprint(matplotlib.matplotlib_fname()) On my computer the above command will print:\n/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc The full path to the font folder can be obtained by replacing /mpl-data/matplotlibrc with /mpl-data/fonts/ttf:\n/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf Next, you can put the font file you would like to use into the font folder, and then update Matplotlib\u0026rsquo;s font manager:\nimport matplotlib.font_manager as fm fm._rebuild() Once this is done, you can check the name of the font you have installed using the following statement. In this example, I have downloaded the font NotoSansCJKtc-Regular.otf from Google Font, and placed it in the file folder.\n[f for f in fm.fontManager.ttflist if \u0026#39;Noto\u0026#39; in f.name] And the following is the output:\n[\u0026lt;Font \u0026#39;Noto Sans CJK TC\u0026#39; (NotoSansCJKtc-Regular.otf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Tagalog\u0026#39; (NotoSansTagalog-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Kayah Li\u0026#39; (NotoSansKayahLi-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Tai Tham\u0026#39; (NotoSansTaiTham-Regular.ttf) normal normal 400 normal\u0026gt;, \u0026lt;Font \u0026#39;Noto Sans Ol Chiki\u0026#39; (NotoSansOlChiki-Regular.ttf) normal normal 400 normal\u0026gt;, ... Here, the name of the font is \u0026ldquo;Noto Sans CJK TC\u0026rdquo;. We can then configure Matplotlib to use this font in our graphs:\nmatplotlib.rcParams[\u0026#39;font.family\u0026#39;] = [\u0026#39;Noto Sans CJK TC\u0026#39;] Below is an example:\nplt.figure(figsize=(8, 2)) plt.bar(x, y) plt.xticks(x, xticks, fontsize=12, rotation=45) plt.title(\u0026#34;åœ–1\u0026#34;, fontsize=18) plt.show() which will produce the same graph as above:\nUsing Custom Fonts in Seaborn Choosing the second method described above allowing you to use the font in Seaborn too. Below is an example that shows how you can configure the font to be used in Seaborn.\nimport seaborn as sns colour = sns.color_palette(\u0026#34;GnBu_d\u0026#34;) sns.set(rc={\u0026#39;figure.figsize\u0026#39;:(8, 2), \u0026#39;figure.dpi\u0026#39;:120}) sns.set(font=\u0026#39;Noto Sans CJK TC\u0026#39;) ax = sns.barplot(xticks, y, palette=colour) ax.set_xticklabels(xticks, rotation=45, fontsize=9) ax.set_title(\u0026#34;åœ–è¡¨1\u0026#34;) ax.grid() And the following graph will be produced:\n","date":"2020-03-15T00:00:00Z","permalink":"https://albertauyeung.github.io/2020/03/15/matplotlib-cjk-fonts.html","title":"ğŸŒ A Guide to Displaying CJK Characters in Matplotlib"},{"content":"PyCon HK 2018 was held on 23-24th November 2018 at Cyberport. I gave a talk on how to deploy machine learning models in Python. The slides of the talk can be found at the link: http://talks.albertauyeung.com/pycon2018-deploy-ml-models/.\nVideo on Youtube ","date":"2018-11-23T00:00:00Z","permalink":"https://albertauyeung.github.io/2018/11/23/pyconhk-ml-deploy.html","title":"ğŸš€ Deploying ML Models in Python - A PyCon HK 2018 Talk"},{"content":"N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\nWhen performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. This post describes several different ways to generate n-grams quickly from input sentences in Python.\nThe Pure Python Way In general, an input sentence is just a string of characters in Python. We can use build in functions in Python to generate n-grams quickly. Let\u0026rsquo;s take the following sentence as a sample input:\ns = \u0026#34;\u0026#34;\u0026#34; Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. \u0026#34;\u0026#34;\u0026#34; If we want to generate a list of bi-grams from the above sentence, the expected output would be something like below (depending on how do we want to treat the punctuations, the desired output can be different):\n[ \u0026#34;natural language\u0026#34;, \u0026#34;language processing\u0026#34;, \u0026#34;processing nlp\u0026#34;, \u0026#34;nlp is\u0026#34;, \u0026#34;is an\u0026#34;, \u0026#34;an area\u0026#34;, ... ] The following function can be used to achieve this:\nimport re def generate_ngrams(s, n): # Convert to lowercases s = s.lower() # Replace all none alphanumeric characters with spaces s = re.sub(r\u0026#39;[^a-zA-Z0-9\\s]\u0026#39;, \u0026#39; \u0026#39;, s) # Break sentence in the token, remove empty tokens tokens = [token for token in s.split(\u0026#34; \u0026#34;) if token != \u0026#34;\u0026#34;] # Use the zip function to help us generate n-grams # Concatentate the tokens into ngrams and return ngrams = zip(*[token[i:] for i in range(n)]) return [\u0026#34; \u0026#34;.join(ngram) for ngram in ngrams] Applying the above function to the sentence, with n=5, gives the following output:\n\u0026gt;\u0026gt;\u0026gt; generate_ngrams(s, n=5) [\u0026#39;natural language processing nlp is\u0026#39;, \u0026#39;language processing nlp is an\u0026#39;, \u0026#39;processing nlp is an area\u0026#39;, \u0026#39;nlp is an area of\u0026#39;, \u0026#39;is an area of computer\u0026#39;, \u0026#39;an area of computer science\u0026#39;, \u0026#39;area of computer science and\u0026#39;, \u0026#39;of computer science and artificial\u0026#39;, \u0026#39;computer science and artificial intelligence\u0026#39;, \u0026#39;science and artificial intelligence concerned\u0026#39;, \u0026#39;and artificial intelligence concerned with\u0026#39;, \u0026#39;artificial intelligence concerned with the\u0026#39;, \u0026#39;intelligence concerned with the interactions\u0026#39;, \u0026#39;concerned with the interactions between\u0026#39;, \u0026#39;with the interactions between computers\u0026#39;, \u0026#39;the interactions between computers and\u0026#39;, \u0026#39;interactions between computers and human\u0026#39;, \u0026#39;between computers and human natural\u0026#39;, \u0026#39;computers and human natural languages\u0026#39;] The above function makes use of the zip function, which creates a generator that aggregates elements from multiple lists (or iterables in genera). The blocks of codes and comments below offer some more explanation of the usage:\n# Sample sentence s = \u0026#34;one two three four five\u0026#34; tokens = s.split(\u0026#34; \u0026#34;) # tokens = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;, \u0026#34;four\u0026#34;, \u0026#34;five\u0026#34;] sequences = [tokens[i:] for i in range(3)] # The above will generate sequences of tokens starting # from different elements of the list of tokens. # The parameter in the range() function controls # how many sequences to generate. # # sequences = [ # [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;], # [\u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;], # [\u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;]] bigrams = zip(*sequences) # The zip function takes the sequences as a list of inputs # (using the * operator, this is equivalent to # zip(sequences[0], sequences[1], sequences[2]). # Each tuple it returns will contain one element from # each of the sequences. # # To inspect the content of bigrams, try: # print(list(bigrams)) # which will give the following: # # [ # (\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;), # (\u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;), # (\u0026#39;three\u0026#39;, \u0026#39;four\u0026#39;, \u0026#39;five\u0026#39;) # ] # # Note: even though the first sequence has 5 elements, # zip will stop after returning 3 tuples, because the # last sequence only has 3 elements. In other words, # the zip function automatically handles the ending of # the n-gram generation. Using NLTK Instead of using pure Python functions, we can also get help from some natural language processing libraries such as the Natural Language Toolkit (NLTK). In particular, nltk has the ngrams function that returns a generator of n-grams given a tokenized sentence. (See the documentaion of the function here)\nimport re from nltk.util import ngrams s = s.lower() s = re.sub(r\u0026#39;[^a-zA-Z0-9\\s]\u0026#39;, \u0026#39; \u0026#39;, s) tokens = [token for token in s.split(\u0026#34; \u0026#34;) if token != \u0026#34;\u0026#34;] output = list(ngrams(tokens, 5)) The above block of code will generate the same output as the function generate_ngrams() as shown above.\n","date":"2018-06-03T00:00:00Z","permalink":"https://albertauyeung.github.io/2018/06/03/generating-ngrams.html","title":"ğŸ Effortlessly Create N-Grams from Text in Python"},{"content":"PyCon HK 2017 was held on 3rd-4th November 2017 at the City University of Hong Kong. I gave a talk on using gradient boosting machines in Python to perform machine learning. The slides of the talk can be found at the link: http://talks.albertauyeung.com/pycon2017-gradient-boosting/.\nVideo on Youtube ","date":"2017-11-05T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/11/05/pyconhk-gbm.html","title":"ğŸš€ Using Gradient Boosting Machines in Python - A PyCon HK 2017 Talk"},{"content":"I gave a talk on deep learning and its applications in a research seminar at the Deep Learning Research \u0026amp; Application Centre (DLC), Hang Seng Management College on 20th July, 2017. The slides of the talk can be found here: http://talks.albertauyeung.com/deep-learning\n","date":"2017-07-21T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/07/21/hsmc-deep-learning-talk.html","title":"ğŸ¤– Deep Learning and Its Applications - Research Seminar at HSMC"},{"content":"pandas is one of the most commonly used Python library in data analysis and machine learning. It is versatile and can be used to handle many different types of data. Before feeding a model with training data, one would most probably pre-process the data and perform feature extraction on data stored as pandas DataFrame. I have been using pandas extensively in my work, and have recently discovered that the time required to manipulate data stored in a DataFrame can vary hugely depending on the method you used.\nNumerical Operations To demonstrate the differences, let\u0026rsquo;s generate some random data first. The following block of code will generate a DataFrame with 5,000 rows and 3 columns (A, B and C) with values ranging from -10 to 10.\nIn [1]: import pandas as pd In [2]: import numpy as np In [3]: data = np.random.randint(-10, 10, (5000, 3)) In [4]: df = pd.DataFrame(data=data, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;], index=None) To track the time required to finish an operation, we can make use of the IPython magic function %timeit to measure the time required to execute a line in Python.\nTo start with, let\u0026rsquo;s consider a simple task of creating a new column in the DataFrame, whose values depend on whether the sum of the values in other columns are greater than zero. First, let\u0026rsquo;s try using the apply function of the DataFrame:\nIn [5]: %timeit df[\u0026#34;D\u0026#34;] = df.apply(lambda x: 1 if x[\u0026#34;A\u0026#34;] + x[\u0026#34;B\u0026#34;] + x[\u0026#34;C\u0026#34;] \u0026gt; 0 else 0, axis=1) 134 ms Â± 1.59 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each) It takes about 134ms to finish the operation, which seems quite fast. However, if we take another approach by using numpy\u0026rsquo;s where() function, we can actually be much faster:\nIn [6]: %timeit df[\u0026#34;E\u0026#34;] = np.where(df[\u0026#34;A\u0026#34;] + df[\u0026#34;B\u0026#34;] + df[\u0026#34;C\u0026#34;] \u0026gt; 0, 1, 0) 757 Âµs Â± 38.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each) This is ~170 times faster! We can verified that the two methods actually give the same results as follows. (np.any checks if any of the values in a list is True).\nIn [7]: np.any(df[\u0026#34;D\u0026#34;] != df[\u0026#34;E\u0026#34;]) False String Operations As another example, let\u0026rsquo;s try searching substrings in a column. Firstly, let\u0026rsquo;s generate some random text data in a new column:\nIn [8]: df[\u0026#34;F\u0026#34;] = np.random.choice([\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;pear\u0026#34;], 5000) Let\u0026rsquo;s say we want to create a new column, whose values depend on whether Column F contains the substring an. Firstly, let\u0026rsquo;s try the apply function:\nIn [9]: %timeit df[\u0026#34;G\u0026#34;] = df.apply(lambda x: 1 if \u0026#34;an\u0026#34; in x[\u0026#34;F\u0026#34;] else 0, axis=1) 61.1 ms Â± 685 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each) Now, if we use the second approach:\nIn [10]: %timeit df[\u0026#34;H\u0026#34;] = np.where(df[\u0026#34;F\u0026#34;].str.contains(\u0026#34;an\u0026#34;), 1, 0) 2.65 ms Â± 40.9 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) which is ~30 times faster.\nThe conclusion is that whenever we can operate on the whole column, we should avoid using apply, which is looping over every row of the DataFrame, and is not able to take advantage of numpy vectorization when performing the calculation.\n","date":"2017-07-08T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/07/08/fast-pandas-operation.html","title":"âš¡ Making pandas Operations Faster"},{"content":"Sequence Labelling in NLP In natural language processing, it is a common task to extract words or phrases of particular types from a given sentence or paragraph. For example, when performing analysis of a corpus of news articles, we may want to know which countries are mentioned in the articles, and how many articles are related to each of these countries.\nThis is actually a special case of sequence labelling in NLP (others include POS tagging and Chunking), in which the goal is to assign a label to each member in the sequence. In the case of identifying country names, we would like to assign a \u0026lsquo;country\u0026rsquo; label to words that form part of a country name, and a \u0026lsquo;irrelevant\u0026rsquo; label to all other words. For example, the following is a sentence broken down into tokens, and its desired output after the sequence labelling process:\ninput = [\u0026#34;Paris\u0026#34;, \u0026#34;is\u0026#34;, \u0026#34;the\u0026#34;, \u0026#34;capital\u0026#34;, \u0026#34;of\u0026#34;, \u0026#34;France\u0026#34;] output = [\u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;C\u0026#34;] where I means that the token of that position is an irrelevant word, and C means that the token of that position is a word that form part of a country name.\nMethods of Sequence Labelling A simple, though sometimes quite useful, approach is to prepare a dictionary of country names, and look for these names in each of the sentences in the corpus. However, this method relies heavily on the comprehensiveness of the dictionary. While there is a limited number of countries, in other cases such as city names the number of possible entries in the dictionary can be huge. Even for countries, many countries may be referred to using different sequence of characters in different contexts. For example, the United States of America may be referred to in an article as the USA, the States, or simply America.\nIn fact, a person reading a news article would usually recognise that a word or a phrase refers to a country, even when he or she has not seen the name of that country before. The reason is that there are many differnt cues in the sentence or the whole article that can be used to determine whether a word or a phrase is a country name. Take the following two sentences as examples:\nKerry travels to Laos\u0026rsquo;s capital, Vientiane, on Monday for meetings of foreign ministers from the 10-member Association of South East Asia Nations (ASEAN). The Governments of Bolivia and Uruguay will strengthen ties with a customs cooperation agreement to be in force on June 15th. The first sentence implies that something called Lao has a capital, suggesting that Lao is a country. Similarly, in the second sentence we know that both Bolivia and Uruguay are countries as the news mentioned about their governments. In other words, the words around \u0026lsquo;Lao\u0026rsquo;, \u0026lsquo;Bolivia\u0026rsquo; and \u0026lsquo;Uruguay\u0026rsquo; provide clues as to whether they are country names.\nConditional Random Field (CRF) To take advantage of the surrounding context when labelling tokens in a sequence, a commonly used method is conditional random field (CRF), first proposed by Lafferty et al. in 2001. It is a type of probabilistic graphical model that can be used to model sequential data, such as labels of words in a sentence.\nThis article is not intended to discuss the technical details of CRF. If you are interested, you are recommended to check out one of the following tutorials which provide very good explanation of how CRF works:\nAn Introduction to Conditional Random Fields by Charles Sutton and Andrew McCallum Introduction to Conditional Random Fields by Edwin Chen In CRF, we will design a set of feature functions to extract features for each word in a sentence. During model training, CRF will try to determine the weights of different feature functions that will maximise the likelihood of the labels in the training data.\nTrain CRF Model in Python One of the commonly used CRF library is CRFSuite implemented by Naoaki Okazaki in C/C++. The library is already easy to use given its command line interface. A Python binding to CRFSuite, pycrfsuite is available for using the API in Python. This Python module is exactly the module used in the POS tagger in the nltk module.\nTo demonstrate how pysrfsuite can be used to train a linear chained CRF sequence labelling model, we will go through an example using some data for named entity recognition.\nNamed Entity Recogniton To train a named entity recognition model, we need some labelled data. The dataset that will be used below is the Reuters-128 dataset, which is an English corpus in the NLP Interchange Format (NIF). It contains 128 economic news articles. The dataset contains information for 880 named entities with their position in the document and a URI of a DBpedia resource identifying the entity. It was created by the Agile Knowledge Engineering and Semantic Web research group at Leipzig University, Germany. More details can be found in their paper.\nIn the following, we will use the XML verison of the dataset, which can be downloaded from https://github.com/AKSW/n3-collection. Below is some lines extracted from the XML data file:\n\u0026lt;document id=\u0026#34;8\u0026#34;\u0026gt; \u0026lt;documenturi\u0026gt;http://www.research.att.com/~lewis/Reuters-21578/15009\u0026lt;/documenturi\u0026gt; \u0026lt;documentsource\u0026gt;Reuters-21578\u0026lt;/documentsource\u0026gt; \u0026lt;textwithnamedentities\u0026gt; \u0026lt;namedentityintext uri=\u0026#34;http://aksw.org/notInWiki/Home_Intensive_Care_Inc\u0026#34;\u0026gt;Home Intensive Care Inc\u0026lt;/namedentityintext\u0026gt; \u0026lt;simpletextpart\u0026gt; said it has opened a Dialysis at Home office in \u0026lt;/simpletextpart\u0026gt; \u0026lt;namedentityintext uri=\u0026#34;http://dbpedia.org/resource/Philadelphia\u0026#34;\u0026gt;Philadelphia\u0026lt;/namedentityintext\u0026gt; \u0026lt;simpletextpart\u0026gt;, its 12th nationwide.\u0026lt;/simpletextpart\u0026gt; \u0026lt;/textwithnamedentities\u0026gt; \u0026lt;/document\u0026gt; The XML block shown above refers to one of the documents in the dataset. The semantics is self-explanatory. The document has a sentence \u0026lsquo;Home Intensive Care Inc said it has opened a Dialysis at Home office in Philadelphia, its 12th nationwide\u0026rsquo;, in which Home Intensive Care Inc and Philadelphia are labelled as named entities.\nPrepare the Dataset for Training In order to prepare the dataset for training, we need to label every word (or token) in the sentences to be either irrelevant or part of a named entity. Since the data is in XML format, we can make use of BeautifulSoup to parse the file and extract the data as follows:\nfrom bs4 import BeautifulSoup as bs from bs4.element import Tag import codecs # Read data file and parse the XML with codecs.open(\u0026#34;reuters.xml\u0026#34;, \u0026#34;r\u0026#34;, \u0026#34;utf-8\u0026#34;) as infile: soup = bs(infile, \u0026#34;html5lib\u0026#34;) docs = [] for elem in soup.find_all(\u0026#34;document\u0026#34;): texts = [] # Loop through each child of the element under \u0026#34;textwithnamedentities\u0026#34; for c in elem.find(\u0026#34;textwithnamedentities\u0026#34;).children: if type(c) == Tag: if c.name == \u0026#34;namedentityintext\u0026#34;: label = \u0026#34;N\u0026#34; # part of a named entity else: label = \u0026#34;I\u0026#34; # irrelevant word for w in c.text.split(\u0026#34; \u0026#34;): if len(w) \u0026gt; 0: texts.append((w, label)) docs.append(texts) The result will be a list of documents, each of which contains a list of (word, label) tuples. For example:\n\u0026gt;\u0026gt;\u0026gt; doc[0][:10] [(\u0026#39;Paxar\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;Corp\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;said\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;it\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;has\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;acquired\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;Thermo-Print\u0026#39;, \u0026#39;N\u0026#39;), ... Generating Part-of-Speech Tags To train a CRF model, we need to create features for each of the tokens in the sentences. One particularly useful feature in NLP is the part-of-speech (POS) tags of the words. They indicates whether a word is a noun, a verb or an adjective. (In fact, a POS tagger is also usually a trained CRF model.)\nWe can use NLTK\u0026rsquo;s POS tagger to generate the POS tags for the tokens in our documents as follows:\nimport nltk data = [] for i, doc in enumerate(docs): # Obtain the list of tokens in the document tokens = [t for t, label in doc] # Perform POS tagging tagged = nltk.pos_tag(tokens) # Take the word, POS tag, and its label data.append([(w, pos, label) for (w, label), (word, pos) in zip(doc, tagged)]) The output of the above process will be a list of documents, each of which is a list of tuples with the word, its POS tag and its label:\n\u0026gt;\u0026gt;\u0026gt; data[0] [(\u0026#39;Paxar\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;Corp\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), (\u0026#39;said\u0026#39;, \u0026#39;VBD\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;it\u0026#39;, \u0026#39;PRP\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;has\u0026#39;, \u0026#39;VBZ\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;acquired\u0026#39;, \u0026#39;VBN\u0026#39;, \u0026#39;I\u0026#39;), (\u0026#39;Thermo-Print\u0026#39;, \u0026#39;NNP\u0026#39;, \u0026#39;N\u0026#39;), ... Generating Features Given the POS tags, we can now continue to generate more features for each of the tokens in the dataset. The features that will be useful in the training process depends on the task at hand. Below are some of the commonly used features for a word $w$ in named entity recognition:\nThe word $w$ itself (converted to lowercase for normalisation) The prefix/suffix of $w$ (e.g. -ion) The words surrounding $w$, such as the previous and the next word Whether $w$ is in uppercase or lowercase Whether $w$ is a number, or contains digits The POS tag of $w$, and those of the surrounding words Whether $w$ is or contains a special character (e.g. hypen, dollar sign) Below is a function for generating features for our documents. It takes a doc (in the form of a listof tuples as shown above), and an index (the $i$th document), and return the documents with features extracted. (A similar example can be found in the repository of pyscrfsuite.)\ndef word2features(doc, i): word = doc[i][0] postag = doc[i][1] # Common features for all words features = [ \u0026#39;bias\u0026#39;, \u0026#39;word.lower=\u0026#39; + word.lower(), \u0026#39;word[-3:]=\u0026#39; + word[-3:], \u0026#39;word[-2:]=\u0026#39; + word[-2:], \u0026#39;word.isupper=%s\u0026#39; % word.isupper(), \u0026#39;word.istitle=%s\u0026#39; % word.istitle(), \u0026#39;word.isdigit=%s\u0026#39; % word.isdigit(), \u0026#39;postag=\u0026#39; + postag ] # Features for words that are not # at the beginning of a document if i \u0026gt; 0: word1 = doc[i-1][0] postag1 = doc[i-1][1] features.extend([ \u0026#39;-1:word.lower=\u0026#39; + word1.lower(), \u0026#39;-1:word.istitle=%s\u0026#39; % word1.istitle(), \u0026#39;-1:word.isupper=%s\u0026#39; % word1.isupper(), \u0026#39;-1:word.isdigit=%s\u0026#39; % word1.isdigit(), \u0026#39;-1:postag=\u0026#39; + postag1 ]) else: # Indicate that it is the \u0026#39;beginning of a document\u0026#39; features.append(\u0026#39;BOS\u0026#39;) # Features for words that are not # at the end of a document if i \u0026lt; len(doc)-1: word1 = doc[i+1][0] postag1 = doc[i+1][1] features.extend([ \u0026#39;+1:word.lower=\u0026#39; + word1.lower(), \u0026#39;+1:word.istitle=%s\u0026#39; % word1.istitle(), \u0026#39;+1:word.isupper=%s\u0026#39; % word1.isupper(), \u0026#39;+1:word.isdigit=%s\u0026#39; % word1.isdigit(), \u0026#39;+1:postag=\u0026#39; + postag1 ]) else: # Indicate that it is the \u0026#39;end of a document\u0026#39; features.append(\u0026#39;EOS\u0026#39;) return features Training the Model To train the model, we need to first prepare the training data and the corresponding labels. Also, to be able to investigate the accuracy of the model, we need to separate the data into training set and test set. Below are some codes for preparing the training data and test data, using the train_test_split function in scikit-learn.\nfrom sklearn.model_selection import train_test_split # A function for extracting features in documents def extract_features(doc): return [word2features(doc, i) for i in range(len(doc))] # A function fo generating the list of labels for each document def get_labels(doc): return [label for (token, postag, label) in doc] X = [extract_features(doc) for doc in data] y = [get_labels(doc) for doc in data] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) In pycrfsuite, A CRF model in can be trained by first creating a trainer, and then submit the training data and corresponding labels to the trainer. After that, set the parameters and call train() to start the training process. For the complete list of parameters, one can refer to the documentation of CRFSuite. With the very small dataset in this example, the training with max_iterations=200 can be finished in a few seconds. Below is the code for creating the trainer and start training the model:\nimport pycrfsuite trainer = pycrfsuite.Trainer(verbose=True) # Submit training data to the trainer for xseq, yseq in zip(X_train, y_train): trainer.append(xseq, yseq) # Set the parameters of the model trainer.set_params({ # coefficient for L1 penalty \u0026#39;c1\u0026#39;: 0.1, # coefficient for L2 penalty \u0026#39;c2\u0026#39;: 0.01, # maximum number of iterations \u0026#39;max_iterations\u0026#39;: 200, # whether to include transitions that # are possible, but not observed \u0026#39;feature.possible_transitions\u0026#39;: True }) # Provide a file name as a parameter to the train function, such that # the model will be saved to the file when training is finished trainer.train(\u0026#39;crf.model\u0026#39;) If you have set verbose=True when initialising the trainer, the trainer will print out the training progress as it is trained against the provided training data.\nChecking the Results Once we have the model trained, we can apply it on our test data and see whether it gives reasonable results. Assuming that the model is saved to a file named crf.model. The following block of code shows how we can load the model into memory, and apply it on to our test data.\ntagger = pycrfsuite.Tagger() tagger.open(\u0026#39;crf.model\u0026#39;) y_pred = [tagger.tag(xseq) for xseq in X_test] # Let\u0026#39;s take a look at a random sample in the testing set i = 12 for x, y in zip(y_pred[i], [x[1].split(\u0026#34;=\u0026#34;)[1] for x in X_test[i]]): print(\u0026#34;%s (%s)\u0026#34; % (y, x)) \u0026#39;\u0026#39;\u0026#39; The following will be printed: sci-med (N) life (N) systems (N) inc (N) said (I) its (I) directors (I) approved (I) a (I) previously (I) ... \u0026#39;\u0026#39;\u0026#39; The result looks reasonable as the first four words are correctly identified as part of a named entity.\nTo study the performance of the CRF tagger trained above in a more quantitative way, we can check the precision and recall on the test data. This can be done very easily using the classification_report function in scikit-learn. However, given that the predictions are sequences of tags, we need to transform the data into a list of labels before feeding them into the function.\nimport numpy as np from sklearn.metrics import classification_report # Create a mapping of labels to indices labels = {\u0026#34;N\u0026#34;: 1, \u0026#34;I\u0026#34;: 0} # Convert the sequences of tags into a 1-dimensional array predictions = np.array([labels[tag] for row in y_pred for tag in row]) truths = np.array([labels[tag] for row in y_test for tag in row]) # Print out the classification report print(classification_report( truths, predictions, target_names=[\u0026#34;I\u0026#34;, \u0026#34;N\u0026#34;])) which will prints a report as follows:\nprecision recall f1-score support I 0.98 0.98 0.98 3322 N 0.85 0.85 0.85 405 avg / total 0.97 0.97 0.97 3727 We can see that we have achieved 85% precision and 85% recall in predicting whether a word is part of a named entity. There are several things by which we can improve the performance, including creating better features or tuning the parameters of the CRF model.\nSource Codes The source code for reproducing the above results can be found in the following github repository: https://github.com/albertauyeung/python-crf-named-entity-recognition.\nReferences Lafferty, J., McCallum, A., Pereira, F. (2001). \u0026ldquo;Conditional random fields: Probabilistic models for segmenting and labeling sequence data\u0026rdquo;. Proc. 18th International Conf. on Machine Learning. Morgan Kaufmann. pp. 282â€“289. Erdogan, H. (2010). Sequence Labeling: Generative and Discriminative Approaches - Hidden Markov Models, Conditional Random Fields and Structured SVMs. ICMLA 2010 Tutorial. ","date":"2017-06-17T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/06/17/python-sequence-labelling-with-crf.html","title":"ğŸ” Performing Sequence Labelling using CRF in Python"},{"content":"There is probably no need to say that there is too much information on the Web nowadays. Search engines help us a little bit. What is better is to have something interesting recommended to us automatically without asking. Indeed, from as simple as a list of the most popular questions and answers on Quora to some more personalized recommendations we received on Amazon, we are usually offered recommendations on the Web.\nRecommendations can be generated by a wide range of algorithms. While user-based or item-based collaborative filtering methods are simple and intuitive, matrix factorization techniques are usually more effective because they allow us to discover the latent features underlying the interactions between users and items. Of course, matrix factorization is simply a mathematical tool for playing around with matrices, and is therefore applicable in many scenarios where one would like to find out something hidden under the data.\nIn this tutorial, we will go through the basic ideas and the mathematics of matrix factorization, and then we will present a simple implementation in Python. We will proceed with the assumption that we are dealing with user ratings (e.g. an integer score from the range of 1 to 5) of items in a recommendation system.\nBasic Idea Just as its name suggests, matrix factorization is to, obviously, factorize a matrix, i.e. to find out two (or more) matrices such that when you multiply them you will get back the original matrix.\nAs mentioned above, from an application point of view, matrix factorization can be used to discover latent features underlying the interactions between two different kinds of entities. (Of course, you can consider more than two kinds of entities and you will be dealing with tensor factorization, which would be more complicated.) And one obvious application is to predict ratings in collaborative filtering.\nIn a recommendation system such as Netflix or MovieLens, there is a group of users and a set of items (movies for the above two systems). Given that each users have rated some items in the system, we would like to predict how the users would rate the items that they have not yet rated, such that we can make recommendations to the users. In this case, all the information we have about the existing ratings can be represented in a matrix. Assume now we have 5 users and 10 items, and ratings are integers ranging from 1 to 5, the matrix may look something like this (a hyphen means that the user has not yet rated the movie):\nD1 D2 D3 D4 U1 5 3 - 1 U2 4 - - 1 U3 1 1 - 5 U4 1 - - 4 U5 - 1 5 4 Hence, the task of predicting the missing ratings can be considered as filling in the blanks (the hyphens in the matrix) such that the values would be consistent with the existing ratings in the matrix.\nThe intuition behind using matrix factorization to solve this problem is that there should be some latent features that determine how a user rates an item. For example, two users would give high ratings to a certain movie if they both like the actors or actresses in the movie, or if the movie is an action movie, which is a genre preferred by both users.\nHence, if we can discover these latent features, we should be able to predict a rating with respect to a certain user and a certain item, because the features associated with the user should match with the features associated with the item.\nIn trying to discover the different features, we also make the assumption that the number of features would be smaller than the number of users and the number of items. It should not be difficult to understand this assumption because clearly it would not be reasonable to assume that each user is associated with a unique feature (although this is not impossible). And anyway if this is the case there would be no point in making recommendations, because each of these users would not be interested in the items rated by other users. Similarly, the same argument applies to the items.\nThe Maths of Matrix Factorization Having discussed the intuition behind matrix factorization, we can now go on to work on the mathematics. Firstly, we have a set $U$ of users, and a set $D$ of items. Let $\\mathbf{R}$ of size $|U| \\times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover $K$ latent features. Our task, then, is to find two matrics $\\mathbf{P}$ (of size $|U| \\times |K|$) and $\\mathbf{Q}$ (of size $|D| \\times |K|$) such that their product apprioximates $\\mathbf{R}$:\n$$\\mathbf{R} \\approx \\mathbf{P} \\times \\mathbf{Q}^T = \\hat{\\mathbf{R}}$$\nIn this way, each row of $\\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\\mathbf{Q}$ would represent the strength of the associations between an item and the features. To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of their vectors:\n$$\\hat{r}_{ij} = p_i^T q_j = \\sum _{k=1}^{K} p _{ik} q _{kj}$$\nNow, we have to find a way to obtain $\\mathbf{P}$ and $\\mathbf{Q}$. One way to approach this problem is the first intialize the two matrices with some values, calculate how different their product is to $\\mathbf{M}$, and then try to minimize this difference iteratively. Such a method is called gradient descent, aiming at finding a local minimum of the difference.\nThe difference here, usually called the error between the estimated rating and the real rating, can be calculated by the following equation for each user-item pair:\n$$e _{ij}^2 = (r _{ij} - \\hat{r} _{ij})^2 = (r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2$$\nHere we consider the squared error because the estimated rating can be either higher or lower than the real rating.\nTo minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. In other words, we need to know the gradient at the current values, and therefore we differentiate the above equation with respect to these two variables separately:\n$$ \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = -2(r _{ij} - \\hat{r} _{ij}) (q _{kj}) = -2 e _{ij} q _{kj} $$\n$$ \\frac{\\partial}{\\partial q _{ik}} e _{ij}^2 = -2(r _{ij} - \\hat{r} _{ij}) (p _{ik}) = -2 e _{ij} p _{ik} $$\nHaving obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:\n$$p\u0026rsquo; _{ik} = p _{ik} + \\alpha \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = p _{ik} + 2\\alpha e _{ij} q _{kj}$$\n$$q\u0026rsquo; _{kj} = q _{kj} + \\alpha \\frac{\\partial}{\\partial q _{kj}} e _{ij}^2 = q _{kj} + 2\\alpha e _{ij} p _{ik}$$\nHere, $\\alpha$ is a constant whose value determines the rate of approaching the minimum. Usually we will choose a small value for $\\alpha$, say 0.0002. This is because if we make too large a step towards the minimum we may run into the risk of missing the minimum and end up oscillating around the minimum.\nA question might have come to your mind by now: if we find two matrices $\\mathbf{P}$ and $\\mathbf{Q}$ such that $\\mathbf{P} \\times \\mathbf{Q}$ approximates $\\mathbf{R}$, isn\u0026rsquo;t that our predictions of all the unseen ratings will be zeros? In fact, we are not really trying to come up with $\\mathbf{P}$ and $\\mathbf{Q}$ such that we can reproduce $\\mathbf{R}$ exactly. Instead, we will only try to minimise the errors of the observed user-item pairs. In other words, if we let $T$ be a set of tuples, each of which is in the form of $(u_i, d_j, r_{ij})$, such that $T$ contains all the observed user-item pairs together with the associated ratings, we are only trying to minimise every $e_{ij}$ for $(u_i, d_j, r_{ij}) \\in T$. (In other words, $T$ is our set of training data.) As for the rest of the unknowns, we will be able to determine their values once the associations between the users, items and features have been learnt.\nUsing the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.\n$$E = \\sum _{(u _i, d _j, r _{ij}) \\in T}{e _{ij}} = \\sum _{(u _i,d _j,r _{ij}) \\in T}{(r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2}$$\nRegularization The above algorithm is a very basic algorithm for factorizing a matrix. There are a lot of methods to make things look more complicated. A common extension to this basic algorithm is to introduce regularization to avoid overfitting. This is done by adding a parameter $\\beta$ and modify the squared error as follows:\n$$e _{ij}^2 = (r _{ij} - \\sum _{k=1}^K p _{ik} q _{kj})^2 + \\frac{\\beta}{2} \\sum _{k=1}^K (||P||^2 + ||Q||^2)$$\nIn other words, the new parameter $\\beta$ is used to control the magnitudes of the user-feature and item-feature vectors such that $P$ and $Q$ would give a good approximation of $R$ without having to contain large numbers. In practice, $\\beta$ is set to some values in the order of 0.02. The new update rules for this squared error can be obtained by a procedure similar to the one described above. The new update rules are as follows:\n$$p\u0026rsquo; _{ik} = p _{ik} + \\alpha \\frac{\\partial}{\\partial p _{ik}} e _{ij}^2 = p _{ik} + \\alpha(2 e _{ij} q _{kj} - \\beta p _{ik} )$$\n$$q\u0026rsquo; _{kj} = q _{kj} + \\alpha \\frac{\\partial}{\\partial q _{kj}} e _{ij}^2 = q _{kj} + \\alpha(2 e _{ij} p _{ik} - \\beta q _{kj} )$$\nAdding Biases When predicting the ratings of users given to items, it is useful to consider how ratings are generated. In the above discussion, we have assumed that ratings are generated based on matching the users preferences on some latent factors and the items\u0026rsquo; characteristics on the latent factors. Actually, it may also be helpful to consider additional factors here.\nFor example, we can assume that when a rating is generated, some biases may also contribute to the ratings. In particular, every user may have his or her own bias, meaning that he or she may tend to rate items higher or lower than the others. In movie ratings, if a user is a serious movie watcher, he or she may tend to give lower ratings, when compared to another user who generally enjoys movies as long as they are not too boring. A similar idea can also be applied to the items being rated.\nHence, in the equal of predicting a rating, we can also add these biases in order to better model how a rating is generated:\nwhere $b$ is the global bias (which can be easily estimated by using the mean of all ratings), $bu_i$ is the bias of user $i$, and $bd_j$ is the bias of item $j$.\nUsing the same steps mentioned above, we can derive the update rules for the user biases and item biases easily:\n$$bu\u0026rsquo; _i = bu _i + \\alpha \\times (e _{ij} - \\beta bu _i)$$\n$$bd\u0026rsquo; _j = bd _j + \\alpha \\times (e _{ij} - \\beta bd _j)$$\nIn practice, the process of factorization will converge faster if biases are included in the model.\nA Simple Implementation in Python Once we have derived the update rules as described above, it actually becomes very straightforward to implement the algorithm. The following is a function that implements the algorithm in Python using the stochastic gradient descent algorithm. Note that this implementation requires the Numpy module.\nimport numpy as np class MF(): def __init__(self, R, K, alpha, beta, iterations): \u0026#34;\u0026#34;\u0026#34; Perform matrix factorization to predict empty entries in a matrix. Arguments - R (ndarray) : user-item rating matrix - K (int) : number of latent dimensions - alpha (float) : learning rate - beta (float) : regularization parameter \u0026#34;\u0026#34;\u0026#34; self.R = R self.num_users, self.num_items = R.shape self.K = K self.alpha = alpha self.beta = beta self.iterations = iterations def train(self): # Initialize user and item latent feature matrice self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K)) self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K)) # Initialize the biases self.b_u = np.zeros(self.num_users) self.b_i = np.zeros(self.num_items) self.b = np.mean(self.R[np.where(self.R != 0)]) # Create a list of training samples self.samples = [ (i, j, self.R[i, j]) for i in range(self.num_users) for j in range(self.num_items) if self.R[i, j] \u0026gt; 0 ] # Perform stochastic gradient descent for number of iterations training_process = [] for i in range(self.iterations): np.random.shuffle(self.samples) self.sgd() mse = self.mse() training_process.append((i, mse)) if (i+1) % 10 == 0: print(\u0026#34;Iteration: %d ; error = %.4f\u0026#34; % (i+1, mse)) return training_process def mse(self): \u0026#34;\u0026#34;\u0026#34; A function to compute the total mean square error \u0026#34;\u0026#34;\u0026#34; xs, ys = self.R.nonzero() predicted = self.full_matrix() error = 0 for x, y in zip(xs, ys): error += pow(self.R[x, y] - predicted[x, y], 2) return np.sqrt(error) def sgd(self): \u0026#34;\u0026#34;\u0026#34; Perform stochastic graident descent \u0026#34;\u0026#34;\u0026#34; for i, j, r in self.samples: # Computer prediction and error prediction = self.get_rating(i, j) e = (r - prediction) # Update biases self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i]) self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j]) # Update user and item latent feature matrices self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:]) self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:]) def get_rating(self, i, j): \u0026#34;\u0026#34;\u0026#34; Get the predicted rating of user i and item j \u0026#34;\u0026#34;\u0026#34; prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T) return prediction def full_matrix(self): \u0026#34;\u0026#34;\u0026#34; Computer the full matrix using the resultant biases, P and Q \u0026#34;\u0026#34;\u0026#34; return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T) We can try to apply it to our example mentioned above and see what we would get. Below is a code snippet in Python for running the example.\nR = np.array([ [5, 3, 0, 1], [4, 0, 0, 1], [1, 1, 0, 5], [1, 0, 0, 4], [0, 1, 5, 4], ]) mf = MF(R, K=2, alpha=0.1, beta=0.01, iterations=20) And the matrix obtained from the above process would look something like this:\n[[ 4.99 3. 3.34 1.01] [ 4. 3.18 2.98 1.01] [ 1.02 0.96 5.54 4.97] [ 1. 0.6 4.78 3.97] [ 1.53 1.05 4.94 4.03]] We can see that for existing ratings we have the approximations very close to the true values, and we also get some \u0026lsquo;predictions\u0026rsquo; of the unknown values. In this simple example, we can easily see that $U1$ and $U2$ have similar taste and they both rated $D1$ and $D2$ high, while the rest of the users preferred $D3$, $D4$ and $D5$. When the number of features ($K$ in the Python code) is 2, the algorithm is able to associate the users and items to two different features, and the predictions also follow these associations. For example, we can see that the predicted rating of $U4$ on $D3$ is 4.78, because $U4$ and $U5$ both rated $D4$ high.\nFurther Information We have discussed the intuitive meaning of the technique of matrix factorization and its use in collaborative filtering. In fact, there are many different extensions to the above technique. An important extension is the requirement that all the elements of the factor matrices $\\mathbf{P}$ and $\\mathbf{Q}$ in the above example) should be non-negative. In this case it is called non-negative matrix factorization (NMF). One advantage of NMF is that it results in intuitive meanings of the resultant matrices. Since no elements are negative, the process of multiplying the resultant matrices to get back the original matrix would not involve subtraction, and can be considered as a process of generating the original data by linear combinations of the latent features.\nSource Code An example can be found at this IPython notebok. It is also available at my Github account in this repository.\nReferences There have been quite a lot of references on matrix factorization. Below are some of the related papers:\nGÃ¡bor TakÃ¡cs et al (2008). Matrix factorization and neighbor based algorithms for the Netflix prize problemIn: Proceedings of the 2008 ACM Conference on Recommender Systems, Lausanne, Switzerland, October 23 - 25, 267-274. Patrick Ott (2008). Incremental Matrix Factorization for Collaborative Filtering. Science, Technology and Design 01/2008, Anhalt University of Applied Sciences. Daniel D. Lee and H. Sebastian Seung (2001). Algorithms for Non-negative Matrix Factorization. Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp. 556â€“562. Daniel D. Lee and H. Sebastian Seung (1999). Learning the parts of objects by non-negative matrix factorization. Nature, Vol. 401, No. 6755. (21 October 1999), pp. 788-791. ","date":"2017-04-23T00:00:00Z","permalink":"https://albertauyeung.github.io/2017/04/23/python-matrix-factorization.html","title":"ğŸ”¥ Matrix Factorization: A Simple Tutorial and Implementation in Python"},{"content":"ä¸€ã€ å…¬å…ƒä¸€ä¹é›¶äº”å¹´ï¼Œæ˜¯ç‰©ç†å­¸ä¸Šå……æ»¿çªç ´çš„ä¸€å¹´ã€‚åœ¨é€™çŸ­çŸ­çš„ä¸€å¹´å…§ï¼Œæ„›å› æ–¯å¦ (Albert Einstein) ç™¼è¡¨äº†äº”ç¯‡æœ‰é—œå…‰é›»ç‰©ç†ï¼Œåˆ†å­é‹å‹•ï¼Œä»¥åŠç›¸å°è«–çš„è«–æ–‡ã€‚äººå€‘æŠŠé€™ä¸€å¹´ç¨±ç‚ºç‰©ç†å­¸æˆ–æ„›å› æ–¯å¦çš„ã€Œå¥‡è¹Ÿå¹´ã€ (Annus Mirabilis)ã€‚åœ¨å·¥ä½œä¹‹é¤˜é€²è¡Œç‰©ç†å­¸ç ”ç©¶çš„é€™æ®µæ™‚é–“ï¼Œæ„›å› æ–¯å¦å±…ä½åœ¨ç‘å£«çš„ä¼¯æ© (Bern) ã€‚ä¼¯æ©æˆç‚ºæ„›å› æ–¯å¦æˆåçš„åœ°æ–¹ï¼Œé€™åŸå¸‚çš„åå­—ï¼Œä¹Ÿå°±è·Ÿé€™ä½å»¿åä¸–ç´€æœ€å‰å¤§ç§‘å­¸å®¶çš„åå­—é€£åœ¨ä¸€èµ·ï¼Œè®Šå¾—ä¸å¯åˆ†å‰²äº†ã€‚\näºŒã€ ç‘å£«æ˜¯ä¸–äººå¿ƒç›®ä¸­çš„æ—…éŠå‹åœ°ï¼Œæ­æ´²äººäº«å—æ»‘é›ªé‹å‹•çš„å¥½åœ°æ–¹ã€‚å…¶ä¸­æ—¥å…§ç“¦ (Geneva) æ˜¯ä¸å°‘åœ‹éš›æ©Ÿæ§‹æˆ–çµ„ç¹”çš„ç¸½éƒ¨æ‰€åœ¨ï¼Œè˜‡é»ä¸– (ZÃ¼rich) å‰‡æ˜¯æ­æ´²é‡è¦çš„é‡‘èä¸­å¿ƒã€‚å¯æ˜¯ï¼Œä½œç‚ºé¦–éƒ½çš„ä¼¯æ©ï¼Œå»æ˜¯å‡ºå¥‡åœ°ä½èª¿ï¼Œå¦‚æœæ²’æœ‰åˆ°éç‘å£«ï¼Œæˆ–æ˜¯å°ç‘å£«ä¸å¤ªç†Ÿæ‚‰çš„äººï¼Œå¤§æ¦‚éƒ½èªªä¸å‡ºé€™å€‹åœ°æ–¹ï¼ŒæŠŠå…¶ä»–å…©å€‹åŸå¸‚ç•¶ä½œç‘å£«çš„é¦–éƒ½çš„ï¼Œä¹Ÿå¤§æœ‰äººåœ¨ã€‚\nä¼¯æ©æ˜¯ç‘å£«ä¼¯æ©å·çš„é¦–åºœï¼Œä½æ–¼ç‘å£«çš„å¾·èªå€ï¼Œæ˜¯ç‘å£«ç¹¼æ—¥å…§ç“¦å’Œè˜‡é»ä¸–å¾Œçš„ç¬¬ä¸‰å¤§åŸå¸‚ã€‚ä¸€å…«å››å…«å¹´ï¼Œç‘å£«æˆç«‹è¯é‚¦æ”¿åºœï¼Œæ–°æ†²æ³•å°‡ä¼¯æ©å®šç‚ºç‘å£«è¯é‚¦é¦–éƒ½ã€‚ç›¸å°æ–¼æ—¥å…§ç“¦å’Œè˜‡é»ä¸–ï¼Œä¼¯æ©ä½œç‚ºç‘å£«çš„è¡Œæ”¿ä¸­å¿ƒçš„ç¢ºæ˜¯ä¸€å€‹æ¯”è¼ƒå…§å‘çš„åŸå¸‚ã€‚ä¸éï¼Œå¾éŠå®¢çš„è§’åº¦ä¾†çœ‹ï¼Œä¼¯æ©å»çµ•ä¸æ¯”å®ƒå€‘éœè‰²ã€‚é™¤äº†åŸå¸‚å››å‘¨çš„è‡ªç„¶é¢¨å…‰ï¼Œåœ¨ä¼¯æ©å¸‚å…§å·²è¢«è¯åˆåœ‹æ•™ç§‘æ–‡çµ„ç¹”ç¢ºèªç‚ºä¸–ç•Œéºç”¢çš„èˆŠåŸå€ï¼Œå¯æ‰¾åˆ°åäº”ä¸–ç´€å®å‰çš„æ­Œå¾·å¼å¤§æ•™å ‚ï¼Œä»¥åŠä¸€å€‹å¤è€çš„å¤§é˜æ¨“ (Zytglogge)ã€‚èˆŠåŸå€çš„å—é¢ï¼Œé‚£å‹’æ²³ (River Aare) çš„å°å²¸ï¼Œæ˜¯ç‘å£«å¹¾æ‰€é‡è¦çš„åšç‰©é¤¨çš„æ‰€åœ¨åœ°ã€‚åŠ ä¸Šæ„›å› æ–¯å¦æ›¾ç¶“åœ¨é€™è£å±…ä½åŠç™¼è¡¨ä»–å­¸è¡“ç”Ÿæ¶¯ä¸­æœ€é‡è¦çš„å¹¾ä»½è«–æ–‡ï¼Œä¼¯æ©å…¶å¯¦æ˜¯ä¸€å€‹éå¸¸å€¼å¾—åˆ°è¨ªçš„åœ°æ–¹ã€‚\nä¸‰ã€ ä¾†ä¼¯æ©æ‰¾æ„›å› æ–¯å¦çš„è¶³è·¡ï¼Œå¯å…ˆåˆ°ä½æ–¼èˆŠåŸå€å…‹æ‹‰å§†å¤§è¡— (Kramgasse) å››åä¹è™Ÿæ„›å› æ–¯å¦çš„æ•…å±…ã€‚æ„›å› æ–¯å¦åœ¨è˜‡é»ä¸–è¯é‚¦å·¥æ¥­å¤§å­¸ (Swiss Federal Institute of Technology ZÃ¼rich) ç•¢æ¥­å¾Œï¼Œè¢«ä¼¯æ©çš„ç‘å£«å°ˆåˆ©å±€è˜ç”¨ç‚ºæŠ€è¡“å“¡ï¼Œå¾äº‹ç”³è«‹å°ˆåˆ©çš„é‘’å®šå·¥ä½œã€‚åœ¨å…‹æ‹‰å§†å¤§è¡—çš„é€™å¹¢æˆ¿å­çš„äºŒæ¨“ï¼Œä¾¿æ˜¯æ„›å› æ–¯å¦å¾ä¸€ä¹é›¶ä¸‰å¹´è‡³ä¸€ä¹é›¶äº”å¹´åœ¨ä¼¯æ©å·¥ä½œæ™‚å±…ä½çš„åœ°æ–¹ã€‚ç¾åœ¨ï¼Œé€™å€‹æˆ¿é–“è¢«ä¿ç•™ä¸‹ä¾†ï¼Œæˆç‚ºä¸€å€‹å°å‹çš„æ„›å› æ–¯å¦åšç‰©é¤¨ã€‚\næ„›å› æ–¯å¦çš„æ•…å±…ï¼Œè·Ÿä¼¯æ©é€™åº§åŸå¸‚ä¸€æ¨£ï¼Œéƒ½å¾ˆä½èª¿ã€‚å…‹æ‹‰å§†å¤§è¡—å››åä¹è™Ÿçš„åœ°ä¸‹ï¼Œæ˜¯ä¸€é–“é¤å»³ã€‚æˆ¿å­æ­£é–€ä¸Šæ¨™è‘—çš„ï¼Œä¾¿æ˜¯é€™é–“é¤å»³çš„åå­—ã€‚ã€Œæ„›å› æ–¯å¦æ•…å±…ã€ (Einstein Haus) çš„å­—å»è¢«å¤¾åœ¨ä¸­é–“ï¼Œè·Ÿå¤–ç‰†çš„é¡è‰²æ›´æ˜¯å·®ä¸å¤šã€‚è‹¥ä¸æ˜¯é †è‘—å¤§è¡—çš„é–€è™Ÿä¸€è·¯æ‰¾ä¾†ï¼Œç›¸ä¿¡å¾ˆå®¹æ˜“ä¾¿æœƒéŒ¯éäº†é€™å€‹åœ°æ–¹ã€‚\næ•…å±…çš„æˆ¿é–“å…§ï¼Œé™³åˆ—è‘—å„ç¨®ä¸åŒçš„ç‰©å“ï¼Œæœ‰æ„›å› æ–¯å¦è®€å¤§å­¸æ™‚çš„æˆç¸¾å–®ã€ä¸åŒçš„è­‰æ›¸çš„å‰¯æœ¬ã€æ„›å› æ–¯å¦çš„ç…§ç‰‡ã€ä»–å…’å­çš„ç¡åºŠã€é‚„æœ‰ä»–ç©¿éçš„ä¸€å¥—è¥¿è£ã€‚æˆ‘æ­£åœ¨åƒè§€çš„æ™‚å€™ï¼Œä¾†äº†ä¸€ç­ä¸­åœ‹çš„ç•™å­¸ç”Ÿï¼Œæœ¬ä¾†å·²ç¶“ä¸æ˜¯å¾ˆå¯¬æ•çš„æˆ¿å­è£ä¸€ä¸‹å­ç†±é¬§èµ·ä¾†ã€‚è² è²¬è³£ç¥¨çš„å¥³å£«è¦‹ä»–å€‘ä¸€å‰¯ååˆ†å¥½å¥‡çš„æ¨£å­ï¼Œèµ°ä¾†ç‚ºä»–å€‘ä»‹ç´¹æˆ¿é–“å…§çš„ç‰©ä»¶ã€‚\næˆ¿é–“é€²é–€å³é‚Šå±•ç¤ºè‘—æ„›å› æ–¯å¦åŠä»–çš„ç¬¬ä¸€ä»»å¦»å­ç±³åˆ—å¨ƒ (Mileva MariÄ‡) åœ¨å¤§å­¸çš„æˆç¸¾å–®ã€‚ç±³åˆ—å¨ƒæ›¾ç¶“åœ¨æ˜¯è˜‡é»ä¸–è¯é‚¦å·¥æ¥­å¤§å­¸ä¿®è®€ç‰©ç†ï¼Œæ˜¯æ„›å› æ–¯å¦çš„åŒå­¸ã€‚é‚£ä½å¥³å£«å‘Šè¨´æˆ‘å€‘ï¼ŒæŠŠä»–å€‘çš„æˆç¸¾å–®æ”¾åœ¨ä¸€èµ·å±•è¦½ï¼Œæ˜¯å¸Œæœ›å¤§å®¶çŸ¥é“ç±³åˆ—å¨ƒåœ¨æ•¸å­¸åŠç‰©ç†å­¸çš„æˆç¸¾ä¹Ÿæ˜¯ååˆ†å‡ºçœ¾çš„ã€‚çš„ç¢ºï¼Œå¾æˆç¸¾å–®ä¸Šå¯è¦‹ï¼Œä»–å€‘å€†ç„¡è«–åœ¨æ•¸å­¸æˆ–ç‰©ç†å­¸çš„ç§‘ç›®ä¸Šå–å¾—å¾ˆå¥½çš„æˆç¸¾ï¼Œå¹¾ä¹æ¯ä¸€ç§‘éƒ½æ‹¿åˆ°æ»¿åˆ†ã€‚å¯èƒ½æ­£å› ç‚ºç±³åˆ—å¨ƒä¹Ÿç²¾é€šæ•¸ç†ï¼Œä¸å°‘äººæå‡ºå¥¹å¯èƒ½åœ¨æ„›å› æ–¯å¦çš„ç ”ç©¶ä¸­ä½œå‡ºä¸å°‘è²¢ç»ã€‚æœ‰äº›äººèªç‚ºç›¸å°è«–çš„åŸºæœ¬ç†è«–æ˜¯ç”±å¥¹æå‡ºçš„ï¼Œæœ‰äº›äººå‰‡èªç‚ºæ„›å› æ–¯å¦æå‡ºäº†ç›¸å°è«–çš„ä¸­å¿ƒæ€æƒ³ï¼Œç±³åˆ—å¨ƒå‰‡å¹«åŠ©ä»–å®Œæˆæ•¸å­¸ä¸Šçš„æ¨è«–ã€‚ä¸éï¼Œé€™äº›èªªæ³•ç„¡å¾ç¨½è€ƒï¼Œä¹Ÿæ²’æœ‰è¢«ä¸–äººæ‰€èªçœŸçœ‹å¾…ã€‚\næˆç¸¾å–®ä¸Šå¦ä¸€å€‹æœ‰è¶£çš„åœ°æ–¹ï¼Œæ˜¯åœ¨çœ¾å¤šç§‘ç›®ä¸­å‡å–å¾—å„ªç•°æˆç¸¾çš„æ„›å› æ–¯å¦ï¼Œç¨åœ¨å…¶ä¸­ä¸€ç§‘ã€Œæ‡‰ç”¨ç‰©ç†å­¸ã€ï¼Œå»æ‹¿äº†æœ€ä½çš„åˆ†æ•¸ã€‚è³£ç¥¨çš„å¥³å£«å‘Šè¨´æˆ‘å€‘ï¼Œå…¶å¯¦æ„›å› æ–¯å¦åœ¨é€™ä¸€ç§‘çš„æˆç¸¾ä¹Ÿæ˜¯å¾ˆå¥½ï¼Œåªæ˜¯ä»–ç¶“å¸¸æ› èª²ï¼Œä»¤è©²èª²çš„æ•™æˆååˆ†ä¸æ»¿ï¼Œæ‰€ä»¥çµ¦ä»–æœ€ä½çš„åˆ†æ•¸ã€‚\nåœ¨å¦ä¸€å€‹æˆ¿é–“å…§ï¼Œæ”¾ç½®äº†ä¸€å¼µæ„›å› æ–¯å¦åœ¨ä¼¯æ©å°ˆåˆ©å±€å·¥ä½œæ™‚æ‰€ä½¿ç”¨çš„æ›¸æ¡Œã€‚åœ¨æ›¸æ¡Œçš„å››å‘¨ï¼Œç‰†ä¸Šå±•ç¤ºè‘—ä¸åŒå­¸è€…åäººå°æ„›å› æ–¯å¦åŠå…¶å°ç‰©ç†å­¸æ‰€ä½œçš„è²¢ç»çš„è©•åƒ¹ï¼Œå…¶ä¸­ä¹Ÿæœ‰å¦‚éœé‡‘ (Stephen Hawking) ç­‰ç‚ºäººæ‰€ç†ŸçŸ¥çš„ç§‘å­¸å®¶ã€‚åœ¨éš”å£çš„æˆ¿é–“ï¼Œæ”¾æ˜ è‘—ä¸€å¥—æœ‰é—œæ„›å› æ–¯å¦ç”Ÿå¹³çš„çŸ­ç‰‡ï¼Œè®“éŠäººå°æ„›å› æ–¯å¦çš„ä¸€ç”Ÿæœ‰å¤šäº›äº†è§£ã€‚\nå››ã€ ä¸€ä¹é›¶äº”å¹´è¢«ç¨±ç‚ºç‰©ç†å­¸ä¸Šçš„ã€Œå¥‡è¹Ÿå¹´ã€ï¼Œé€™æ˜¯ç”±æ–¼æ„›å› æ–¯å¦åœ¨é€™çŸ­çŸ­ä¸€å¹´å…§æ‰€ç™¼è¡¨çš„ä¸€å…±äº”ç¯‡è«–æ–‡ï¼Œå°ç‰©ç†å­¸ã€å¤©æ–‡å­¸ã€ä»€è‡³æ•´å€‹ç§‘å­¸ç•Œä½œå‡ºäº†è«å¤§çš„è²¢ç»ã€‚ä»¥ä¸‹æ˜¯é€™äº”ç¯‡è«–æ–‡çš„é¡Œç›®ã€‚\nã€Šé—œæ–¼å…‰çš„ç”¢ç”Ÿå’Œè½‰åŒ–çš„ä¸€å€‹å•Ÿç™¼æ€§è§€é»ã€‹(1) ã€Šæ¸¬å®šåˆ†å­å¤§å°çš„æ–°æ–¹æ³•ã€‹(2) ã€Šæ ¹æ“šåˆ†å­é‹å‹•è«–ç ”ç©¶éœæ­¢æ¶²é«”ä¸­æ‡¸æµ®å¾®ç²’çš„é‹å‹•ã€‹(3) ã€Šè«–é‹å‹•ç‰©é«”çš„é›»å‹•åŠ›å­¸ã€‹(4) ã€Šç‰©é«”æ…£æ€§èˆ‡å…¶æ‰€å«èƒ½é‡æœ‰é—œå—ï¼Ÿã€‹(5) é€™äº”ç¯‡è«–æ–‡ä¸­çš„å…¶ä¸­ä¸€ç¯‡ï¼Œã€Šæ¸¬å®šåˆ†å­å¤§å°çš„æ–°æ–¹æ³•ã€‹ï¼Œæ˜¯æ„›å› æ–¯å¦å‘è˜‡é»ä¸–å¤§å­¸ (University of Zurich) æäº¤çš„åšå£«è«–æ–‡ã€‚åˆ¥ä»¥ç‚ºåšå£«è«–æ–‡ä¸€å®šæ˜¯é•·ç¯‡å¤§è«–ã€é æ•¸éç™¾çš„æ–‡ç« ï¼Œæ„›å› æ–¯å¦é€™ç¯‡è«–æ–‡åªæœ‰äºŒåä¸€é ï¼æ ¹æ“šæ„›å› æ–¯å¦æ‰€è¿°ï¼Œä»–ç¬¬ä¸€æ¬¡æäº¤é€™ç¯‡è«–æ–‡æ™‚ï¼Œå¤§å­¸å›è¦†èªªç¯‡å¹…å¤ªçŸ­ï¼Œè¦ä»–ä½œå‡ºä¿®æ”¹ã€‚å¯æ˜¯ï¼Œæœ€çµ‚æ„›å› æ–¯å¦ä¹Ÿåªæ˜¯åœ¨åŸç¨¿ä¸­å¤šåŠ äº†ä¸€å¥å¥å­ï¼Œä¾¿å†æ¬¡æäº¤ï¼Œä¸¦ä¸”é€šéå¯©æ ¸ï¼Œç²é ’ç™¼åšå£«å­¸ä½ã€‚\né™¤ä»–çš„åšå£«è«–æ–‡å¤–ï¼Œæ„›å› æ–¯å¦åœ¨é€™ä¸€å¹´å…§ï¼ŒæŠŠå…¶ä»–å¹¾ç¯‡è«–æ–‡åœ¨ä¸€æœ¬çŸ¥åçš„ç‰©ç†å­¸å­¸è¡“æœŸåˆŠï¼ŒAnnalen der Physik (ã€Šç‰©ç†å­¸å¹´é‘‘ã€‹) ï¼Œé€ä¸€ç™¼è¡¨(ä»–çš„åšå£«è«–æ–‡ç¶“ä¿®æ”¹å¾Œåœ¨æ¬¡å¹´ä¹Ÿè¢«åˆŠç™»æ–¼é€™æœ¬æœŸåˆŠä¸­)ã€‚é€™äº›è«–æ–‡ä¸€ç¶“ç™¼è¡¨ï¼Œæ•´å€‹ç‰©ç†å­¸ç•Œéƒ½é©šè¨ä¸å·²ã€‚ä»–å€‘çš„é©šè¨ï¼Œä¸å–®åœ¨æ–¼é€™å¹¾ç¯‡è«–æ–‡æ‰€æ¶‰åŠçš„å»£æ³›é¡Œç›®ä»¥åŠç¨åˆ°å’Œå‰µæ–°çš„å…§å®¹ï¼Œè€Œæ›´åœ¨æ–¼é€™äº›è«–æ–‡ç«Ÿç„¶æ˜¯ç”±æ„›å› æ–¯å¦â”€â”€ä¸€å€‹ä¸¦éåœ¨å¤§å­¸å·¥ä½œåŠç ”ç©¶ã€é‚„æœªå–å¾—åšå£«å­¸ä½ã€åªåœ¨å°ˆåˆ©å±€å·¥ä½œçš„æŠ€è¡“å“¡â”€â”€æ‰€ç™¼è¡¨ã€‚æ„›å› æ–¯å¦å¹¾ä¹èˆ‡è·Ÿç•¶æ™‚çš„å­¸è€…æ²’æœ‰äº¤æµï¼Œç§‘å­¸ä¸Šçš„è¨è«–ä¹Ÿåªé™æ–¼è·Ÿä»–ç§äººæˆèª²çš„å­¸ç”Ÿä»¥åŠä»–çš„å¦»å­çš„è«‡è©±ã€‚ä»–çš„è³‡æºç›¸å°åœ¨å¤§å­¸å·¥ä½œçš„å­¸è€…ä¹Ÿè¼ƒç‚ºè²§ä¹ï¼Œå¯ä¾›é–±è®€çš„æ›¸ç±åªé™æ–¼å°ˆåˆ©å±€åœ–æ›¸é¤¨å’Œä»–è‡ªå·±çš„è—æ›¸ï¼Œè€Œæ²’æœ‰å¤§å­¸åœ–æ›¸é¤¨é‚£ç¨®å®Œå‚™çš„æ”¯æ´ã€‚ç„¶è€Œï¼Œé€™å¹¾ç¯‡è«–æ–‡æ‰€æå‡ºçš„ï¼Œå»æ˜¯èƒ½æ”¹è®Šæˆ‘å€‘å°é€™å€‹å®‡å®™çš„ç†è§£çš„é‡è¦ç™¼ç¾ã€‚\nä¿®è®€éé«˜ä¸­ç‰©ç†å­¸çš„ï¼Œä¸€å®šçŸ¥é“ç‰©ç†å­¸ä¸Šçš„ã€Œå…‰é›»æ•ˆæ‡‰ã€ (photoelectric effect) ã€‚ä¸Šè¿°çš„å…¶ä¸­ä¸€ç¯‡è«–æ–‡ï¼Œã€Šé—œæ–¼å…‰çš„ç”¢ç”Ÿå’Œè½‰åŒ–çš„ä¸€å€‹å•Ÿç™¼æ€§è§€é»ã€‹ï¼Œæ­£è¨˜è¼‰è‘—æ„›å› æ–¯å¦å°å…‰é›»æ•ˆæ‡‰ä½œå‡ºåˆ†æçš„çµæœã€‚ç›´è‡³åä¹ä¸–ç´€æœ«æœŸï¼Œç‰©ç†å­¸ç•Œä¸€ç›´èªç‚ºå…‰æ˜¯ä¸€ç¨®èƒ½é‡ï¼Œä¸¦ä»¥é€£çºŒä¸ç¹¼çš„å…‰æ³¢ (wave) çš„å½¢å¼å­˜åœ¨ã€‚å¯æ˜¯ï¼Œç•¶æ™‚ä¸€äº›æœ‰é—œå…‰é›»æ•ˆæ‡‰â”€â”€æŒ‡é‡‘å±¬åœ¨å…‰çš„ç…§å°„ä¸‹ç™¼é‡‹æ”¾å‡ºè‡ªç”±é›»å­â”€â”€çš„å¯¦é©—ä¸­ï¼Œå»å‡ºç¾äº†å¾ˆå¤šèˆ‡æ³¢å‹•ç†è«–äº’ç›¸çŸ›ç›¾çš„çµæœã€‚æ„›å› æ–¯å¦åœ¨é€™ç¯‡è«–æ–‡ä¸­ï¼Œæå‡ºå…‰é‡å­ (photon) çš„æ¦‚å¿µï¼Œä¸¦åˆ©ç”¨æ•¸å­¸ä¸Šçš„æ¨è«–ï¼Œç²¾å½©åœ°è§£é‡‹äº†å…‰é›»æ•ˆæ‡‰çš„åŸç†ã€‚çµæœï¼Œæ­¤ç†è«–ä¸å–®ç‚ºç‰©ç†å­¸ç•Œæ‰€æ¥ç´ï¼Œæ›´ç‚ºæ„›å› æ–¯å¦å¸¶ä¾†ä¸€ä¹äºŒä¸€å¹´çš„è«¾è²çˆ¾ç‰©ç†å­¸çã€‚\nç•¶ç„¶ï¼Œæ¯”èµ·å…‰é›»æ•ˆæ‡‰ï¼Œæ„›å› æ–¯å¦æ‰€å‰µç«‹çš„ç›¸å°è«–æ›´ç‚ºäººæ‰€ç†ŸçŸ¥ã€‚ã€Šè«–é‹å‹•ç‰©é«”çš„é›»å‹•åŠ›å­¸ã€‹é€™ç¯‡è«–æ–‡æ­£æ˜¯ç™¼è¡¨ä»–åœ¨ç‹¹ç¾©ç›¸å°è«–çš„ç ”ç©¶æˆæœã€‚å…¶å¯¦ï¼Œæ„›å› æ–¯å¦æ›¾ç¶“å¤šæ¬¡å› ç‚ºç›¸å°è«–çš„ç ”ç©¶è¢«æåç‚ºè«¾è²çˆ¾å¾—çè€…ã€‚å¯æ˜¯ï¼Œç”±æ–¼ç•¶æ™‚è©•å¯©å§”å“¡æœƒå°ç†è«–ç‰©ç†å­¸è¼ƒç‚ºæŠ—æ‹’ï¼ŒåŠ ä¸Šç›¸å°è«–é›£ä»¥è¢«ç†è§£ï¼Œåˆç¼ºä¹æœ‰åŠ›çš„è­‰æ“šï¼Œå› æ­¤æ„›å› æ–¯å¦å¾æœªå› ç‚ºç›¸å°è«–è€Œç²å¾—è«¾è²çˆ¾çã€‚æ„›å› æ–¯å¦å¦ä¸€å»£ç‚ºäººçŸ¥çš„ç™¼ç¾ï¼Œæ˜¯ã€Œèƒ½é‡â”€â”€ç‰©è³ªè½‰æ›æ–¹ç¨‹å¼ã€ï¼Œå³ E=mc2ã€‚ã€Šç‰©é«”æ…£æ€§èˆ‡å…¶æ‰€å«èƒ½é‡æœ‰é—œå—ï¼Ÿã€‹é€™ç¯‡è«–æ–‡æ­£æ˜¯æ¨å°å‡ºé€™ä¸€æ–¹ç¨‹å¼çš„åŸºç¤ç ”ç©¶ã€‚\näº”ã€ æ„›å› æ–¯å¦å°ç§‘å­¸æœ‰è‘—é‡å¤§çš„è²¢ç»ï¼ŒåŒæ™‚äº¦æ˜¯ä¸€å€‹å‚³å¥‡çš„äººç‰©ï¼Œå°å°çš„ä¸€æ‰€æˆ¿å­ï¼Œç•¶ç„¶ä¸è¶³ä»¥è®“äººå€‘äº†è§£ä»–çš„æ•…äº‹ã€‚é™¤äº†é€™ã€Œæ„›å› æ–¯å¦æ•…å±…ã€å¤–ï¼Œä¼¯æ©çš„æ­·å²åšç‰©é¤¨ä¸­ï¼Œé‚„è¨­æœ‰ä¸€å€‹è³‡æ–™éå¸¸è©³ç›¡çš„æ„›å› æ–¯å¦å±•è¦½ï¼Œä»‹ç´¹æ„›å› æ–¯å¦çš„ç”Ÿå¹³ã€‚é€™å€‹å±•è¦½ï¼Œæœ¬ä¾†æ˜¯ä¼¯æ©çš„æ­·å²åšç‰©é¤¨åœ¨äºŒé›¶é›¶äº”å¹´æ™‚ï¼Œç‚ºæ…¶ç¥æ„›å› æ–¯å¦æå‡ºç›¸å°è«–ä¸€ç™¾é€±å¹´è€Œç±ŒåŠƒçš„ä¸€å€‹ç‰¹å‚™å±•è¦½ï¼Œä¸¦ä¸æ˜¯å¸¸è¨­çš„å±•è¦½ã€‚å¯æ˜¯ï¼Œè‡ªå±•è¦½è¨­ç«‹å·²ä¾†ï¼Œä¾†åƒè§€çš„äººæ•¸ä¸æ–·å¢åŠ ï¼Œä¸¦å»£å—æœ¬åœ°äººåŠéŠå®¢çš„æ­¡è¿ï¼Œæ‰€ä»¥æ­·å²åšç‰©é¤¨æ±ºå®šä¿ç•™é€™å€‹å±•è¦½ï¼Œåˆä¹¾è„†æŠŠå®ƒå‘½åç‚ºã€Œæ„›å› æ–¯å¦åšç‰©é¤¨ã€ã€‚\nã€Œæ„›å› æ–¯å¦åšç‰©é¤¨ã€æ‰€æ”¶è—çš„å±•å“ååˆ†è±å¯Œï¼Œå°æ„›å› æ–¯å¦ç”Ÿå¹³çš„æè¿°ä¹Ÿååˆ†ä»”ç´°ã€‚æ„›å› æ–¯å¦ä¸åƒ…æ˜¯ä¸€å€‹å‚³å¥‡äººç‰©ï¼Œä»–çš„ä¸€ç”Ÿï¼Œä¹Ÿè¦‹è­‰è‘—æ­·å²ä¸Šå¾ˆå¤šå¦‚å…©æ¬¡ä¸–ç•Œå¤§æˆ°ç­‰é‡å¤§çš„äº‹æ•…æˆ–è®Šé·ï¼Œå±•è¦½ä¸­å°é€™äº›äº‹ä»¶ä¹Ÿæœ‰è‘—é —ç‚ºæ·±å…¥çš„ä»‹ç´¹ã€‚çœ‹éé€™å€‹å±•è¦½å¾Œï¼Œä¸å–®å°æ„›å› æ–¯å¦çš„å€‹äººç”Ÿæ´»ã€å­¸è¡“ç”Ÿæ¶¯åŠäººç”Ÿå“²å­¸æœ‰äº†æ›´æ·±åˆ»çš„äº†è§£ï¼Œå°æ–¼ä»–çš„æ™‚ä»£å’Œæ­·å²å¤§äº‹çš„çŸ¥è­˜ä¹Ÿæœƒå¢é€²ä¸å°‘ã€‚åƒè§€è€…ç”±æ„›å› æ–¯å¦çš„å‡ºç”Ÿé–‹å§‹ï¼Œæ…¢æ…¢äº†è§£ä»–çš„ç«¥å¹´ã€ä»–çš„æ•™è‚²ã€ä»–åœ¨ç‘å£«çš„ç”Ÿæ´»ã€å› ç‰©ç†å­¸ä¸Šçš„ç ”ç©¶ç²å¾—è«¾è²çˆ¾çï¼Œä»¥åŠä»–åˆ°ç¾åœ‹æ™®æ—æ–¯é “å®šå±…å¾Œçš„ç”Ÿæ´»ã€‚ç•¶åƒè§€è€…èµ°åˆ°å±•è¦½çš„ç›¡é ­ï¼Œä¹Ÿæ˜¯æ„›å› æ–¯å¦ç”Ÿå‘½çš„çµ‚çµã€‚ä¸€ä¹äº”äº”å¹´å››æœˆåå…«æ—¥æ„›å› æ–¯å¦æ–¼æ™®æ—æ–¯é “å»ä¸–ã€‚ä»–è‡¨çµ‚æ™‚èªªäº†å¹¾å¥è©±ï¼Œéƒ½æ˜¯ç”¨ä»–è¦ºå¾—æœ€è¦ªåˆ‡çš„å¾·èªèªªçš„ï¼Œç•¶æ™‚åœ¨ä»–èº«æ—çš„è¬¢å£«ä¸è«³å¾·èªï¼Œæ²’èƒ½è½æ‡‚ä»–æœ€å¾Œçš„èªªè©±ã€‚\nåƒè€ƒè³‡æ–™ ä¼¯æ© Bern - Official Web Site Bern Einstein House Bern History Museum æ„›å› æ–¯å¦ Albert Einstein - Wikipedia Annus Mirabilis Papers - Wikipedia Einstein Year 2005 Einstein Archives Online TIME Magazine 1st July 1946 issue TIME 100: Albert Einstein Michael White and John Gribbin. 1993. Einstein - A Life in Science. London: Simon \u0026amp; Schuster UK Ltd. æ„›å› æ–¯å¦ã€Œå¥‡è¹Ÿå¹´ã€è«–æ–‡ (Annus Mirabilis Papers) \u0026ldquo;On a heuristic viewpoint concerning the production and transformation of light.\u0026rdquo; Annalen der Physik, 17:132-148, 1905. \u0026ldquo;A new determination of molecular dimensions. University of Zurich, Ph.D. Dissertation, 30 April 1905. \u0026ldquo;On the motion of small particles suspended in liquids at rest required by the molecular-kinetic theory of heat.\u0026rdquo; Annalen der Physik, 17:549-560, 1905. \u0026ldquo;On the electrodynamics of moving bodies.\u0026rdquo; Annalen der Physik. 17:891-921, 1905. \u0026ldquo;Does the inertia of a body depend upon its energy content?\u0026rdquo; Annalen der Physik, 18:639-641. 1905. æœ‰é—œæ„›å› æ–¯å¦çš„æ–‡å­¸ä½œå“ Alan Lightman. 1993. Einstein\u0026rsquo;s Dream. New York: Pantheon Books. Jean-Claude Carrier. 2005. Please, Mr Einstein (Einstein S\u0026rsquo;il Vous Plait). London: Harvill Secker. ","date":"2007-05-04T00:00:00Z","image":"https://albertauyeung.github.io/images/einstein_bern_01.jpg","permalink":"https://albertauyeung.github.io/2007/05/04/einstein-house.html","title":"ğŸ§  æ„›å› æ–¯å¦èˆ‡ä¼¯æ©"},{"content":"Chateau de chillon çŸ³åº¸å¤å ¡ å¾æ´›æ¡‘ (Lausanne) åç«è»Šåˆ°è’™ç‰¹å‹’ (Montreux)ï¼Œæ²¿è‘—æ—¥å…§ç“¦æ¹– (Lac LÃ©man) é‚Šæœ›å»ï¼Œå¯ä»¥çœ‹åˆ°ä¸€åº§ç¢©å¤§çš„åŸå ¡å€šç«‹åœ¨å²¸é‚Šã€‚é€™åº§å¤å ¡ï¼Œåæ°£ä¸å°ã€‚å¤å ¡å¤è€èŠåš´çš„å»ºç¯‰åŠå››å‘¨ç¶ºéº—çš„é¢¨å…‰è‡ªç„¶å¸å¼•ä¸å°‘éŠäººï¼Œå®ƒé‚„å› ç‚ºè‹±åœ‹è©©äººæ‹œå€« (Lord Byron) çš„ä¸€é¦–è©©è€Œåè²å¤§å™ªã€‚é€™åº§å¤å ¡ï¼Œä¾¿æ˜¯ ChÃ¢teau de chillon.\nå¤å ¡çš„ä¸­æ–‡è­¯åä¼¼ä¹æ²’æœ‰è¢«çµ±ä¸€ï¼Œå”®ç¥¨è·å“¡çµ¦æˆ‘çš„å°å†Šå­ä¸Šå¯«è‘—ã€ŒçŸ³åº¸å¤å ¡ã€ï¼Œå¯æ˜¯å¤å ¡å¤–çš„å°å•†åº—å…§å”®è³£çš„æ›¸ç±å»å«å®ƒä½œã€Œè¥¿åº¸å¤å ¡ã€ï¼Œæˆ‘åœ¨ç¶²è·¯ä¸Šä¹Ÿçœ‹éä¸å°‘å…¶ä»–çš„è­¯åï¼Œå¦‚ã€Œè©©åº¸ã€ã€ã€Œå¸Œéš†ã€ç­‰ç­‰ã€‚è‹¥æŠŠé€™äº›åå­—è·Ÿå®ƒçš„æ³•èªåå­—ä½œæ¯”è¼ƒï¼Œä¹Ÿå¯¦åœ¨æ²’æœ‰å¤ªå¤§çš„åˆ†åˆ¥ã€‚ç„¡è«–å¦‚ä½•ï¼Œå°å†Šå­çš„è­¯åè©²å¯ç®—æ˜¯æ¯”è¼ƒã€Œå®˜æ–¹ã€çš„å§ï¼Œæš«ä¸”ç”¨ã€ŒçŸ³åº¸å¤å ¡ã€é€™å€‹åå­—å§ã€‚\nåˆ°çŸ³åº¸å¤å ¡ï¼Œåç«è»Šåˆ° Veytaux-Chillon ç«™ç®—æ˜¯æœ€æ–¹ä¾¿çš„ï¼Œä¸‹è»Šå¾Œåœ¨æ—¥å…§ç“¦æ¹–é‚Šæ­¥è¡Œ 10 åˆ†é˜å·¦å³ä¾¿å¯åˆ°é”ã€‚å¯æ˜¯ï¼Œé€™ç•¢ç«Ÿæ˜¯ä¸€å€‹å°è»Šç«™ï¼Œä¾†å¾€å¤§åŸå¸‚çš„åˆ—è»Šä¸åœæ–¼æ­¤ã€‚å¦ä¸€å€‹æ–¹æ³•æ˜¯åç«è»Šåˆ°è’™ç‰¹å‹’ï¼Œå¾ç«è»Šç«™åå·´å£«ä¾†åˆ°åŸå ¡çš„é–€å‰ä¸‹è»Šã€‚\né›–èªªæ‹œå€«çš„è©©ç‚ºé€™åº§å¤å ¡æ·»äº†ä¸å°‘åæ°£ï¼Œå¯æ˜¯å®ƒæœ¬èº«å°éŠäººä¾†èªªä¹Ÿæ˜¯ååˆ†å¸å¼•çš„ã€‚ä¸€èˆ¬åŸå ¡ï¼Œå¤šå»ºåœ¨å±±ä¸˜é«˜åœ°ä¸Šï¼Œå±…é«˜è‡¨ä¸‹ï¼Œä»¥ä¾¿é˜²ç¦¦å¤–ä¾†çš„è¥²æ“Šã€‚ä½†æ˜¯ï¼ŒçŸ³åº¸å¤å ¡å»å»ºåœ¨æ¹–é‚Šçš„ä¸€å€‹å°å³¶ä¸Šï¼Œæµéå²¸é‚Šå’Œå³¶çŸ³é–“çš„æ—¥å…§ç“¦æ¹–æ°´ä¾¿æˆäº†å®ƒè‡ªç„¶çš„è¬¢åŸæ²³ã€‚æ—¥å…§ç“¦æ¹–å¹³éœå¦‚é¡ï¼Œåœ¨å¤å ¡é«˜è™•ä¾ç¨€å¯è¦‹æ¹–çš„å½¼å²¸ï¼Œå››å‘¨æ˜¯é«˜è³å…¥é›²çš„é›ªå±±ï¼Œæ˜ åœ¨æ¹–é¢ä¸Šï¼Œæ›´è¦ºå®å‰ã€‚\nçŸ³åº¸å¤å ¡çš„æ­·å²ï¼Œæœ€æ—©çš„è¨˜è¼‰å§‹è‡ªå…¬å…ƒä¸€é›¶é›¶äº”å¹´ã€‚å¯æ˜¯ï¼Œå¤å ¡çš„åœç‰†å¯è¿½æº¯è‡³ä¸­ä¸–ç´€æ™‚ä»£ã€‚å¤å ¡æ‰€åœ¨çš„å°å³¶ï¼Œæ›´å› åœ¨åŸå ¡å…¥å£é™„è¿‘æ‰€ç™¼æ˜åˆ°çš„å¢³å¢“ï¼Œè¢«è­‰å¯¦åœ¨é’éŠ…æ™‚ä»£å·²ç¶“æœ‰äººé¡èšå±…ã€‚åœ¨é™„è¿‘ç™¼æ˜åˆ°çš„å…¶ä»–æ–‡ç‰©ï¼Œä¹Ÿè­‰æ˜ç¾…é¦¬äººæ›¾ç¶“ä½”é ˜æ­¤åœ°ã€‚çŸ³åº¸å¤å ¡ä½è™•ç‘å£«é€šå¾€æ„å¤§åˆ©çš„å±±é“ï¼Œè‡ªå¤ä»¥ä¾†å³æ˜¯è»äº‹è¦åœ°ã€‚å¤å ¡æœ¬ä¾†ç‚ºéŒ«æ°¸ä¸»æ•™(Bishops of Sion) æ‰€æœ‰ï¼Œè‡ªåäºŒä¸–ç´€èµ·å‰‡æˆç‚ºè–©ä¼ä¾å…¬çˆµ (Dukes of Savoy) çš„è²¡ç”¢ï¼Œç›´åˆ°ä¸€äº”ä¸‰å…­å¹´ç‚ºç‘å£«äººæ‰€æ”»ä½”ã€‚ç‘å£«ä¼¯æ© (Bern) çš„è»éšŠä½”é ˜åŸå ¡å¾Œï¼Œæ›¾ç¶“ç”¨ä½œå¤§æ³•å®˜çš„å€‰åº«ã€è»ç«åº«ã€åºœç¬¬ï¼Œä»¥åŠç›£ç„ã€‚å¤å ¡çš„å¡”æ¨“å’ŒåŸå›ä¾¿æ˜¯åœ¨é€™å€‹æ™‚æœŸå»ºé€ çš„ã€‚è‡³ä¸€ä¸ƒä¹å…«å¹´ï¼Œæ²ƒå· (Canton of Vaud) å–å¾—ç¨ç«‹ï¼ŒçŸ³åº¸å¤å ¡å¾æ­¤æˆç‚ºæ²ƒå·çš„è²¡ç”¢ã€‚ä¸€å…«å…«ä¸ƒå¹´ï¼Œç‘å£«æˆç«‹äº†ã€ŒçŸ³åº¸å¤å ¡ä¿®å¾©å”æœƒã€ (Association du ChÃ¢teau de Chillon)ï¼Œå°ˆè²¬å¤å ¡çš„ä¿®å¾©åŠåœè¬¢å·¥ä½œã€‚å¾å®ƒçš„æ­·å²ï¼Œå¯è¦‹çŸ³åº¸å¤å ¡æ›¾ç¶“å¤šæ¬¡æ˜“æ‰‹ï¼Œè€Œä¸åŒçš„ä¸»äººä¹Ÿæ›¾ç¶“é€²è¡Œå¤šæ¬¡ä¿®å¾©åŠå¢å»ºçš„å·¥ä½œã€‚æ­£å› å¦‚æ­¤ï¼ŒçŸ³åº¸å¤å ¡çš„ä¸åŒéƒ¨ä»½çš„å»ºç¯‰é›–ç„¶è‰²èª¿ä¸€è‡´ï¼Œä½†é¢¨æ ¼å»æ˜¯æœ‰æ‰€ä¸åŒï¼Œé€™ä¹Ÿæˆç‚ºäº†å¤å ¡çš„ç‰¹è‰²ä¹‹ä¸€ã€‚\nçŸ³åº¸å¤å ¡æˆç‚ºå„åœ‹éŠå®¢è¶¨ä¹‹è‹¥é¶©çš„ç‘å£«æ™¯é»ï¼Œä¹ƒæ˜¯å› ç‚ºåä¹ä¸–ç´€è‹±åœ‹æµªæ¼«è©©äººæ‹œå€«æ‰€å¯«çš„ä¸€é¦–é¡Œç‚ºã€ˆçŸ³åº¸çš„å›šå¾’ã€‰ (The Prisoner of Chillon) çš„è©©ã€‚ä¸€å…«ä¸€å…­å¹´ï¼Œæ‹œå€«è·Ÿå±…ä½åœ¨ç‘å£«çš„å‹äººã€è‹±åœ‹è©©äººé›ªèŠ (Shelley) ä¸€åŒéŠè¦½æ—¥å…§ç“¦æ¹–ï¼Œä¸¦åˆ°çŸ³åº¸å¤å ¡åƒè§€ã€‚æ‹œå€«åœ¨åƒè§€æœŸé–“ï¼Œå¾—çŸ¥æ—¥å…§ç“¦ä¸€ä½æ°‘æ—è‹±é›„æ³¢å°¼ä¼ (FranÃ§ois Bonivard) å› æ”¯æŒæ—¥å…§ç“¦ç¨ç«‹è€Œè¢«å›šç¦æ–¼å¤å ¡åœ°ä¸‹å›šç‰¢çš„äº‹è¹Ÿï¼Œæœ‰æ„Ÿè€Œç™¼å¯«ä¸‹ä¸€é¦–é¡Œç‚º \u0026ldquo;Sonnet on Chillon\u0026rdquo; çš„è©©æ­Œã€‚å…¶å¾Œåœ¨æ´›æ¡‘æš«å±…çš„æ‹œå€«ï¼ŒåŸºæ–¼æ³¢å°¼ä¼çš„äº‹è¹Ÿï¼Œç¹¼çºŒå®Œæˆäº†é•·è©©ã€ˆçŸ³åº¸çš„å›šå¾’ã€‰ï¼Œä¸¦åœ¨å›åˆ°è‹±åœ‹å¾Œå‡ºç‰ˆã€‚\næ‹œå€«æ‰€åˆ°éçš„å›šç¦æ³¢å°¼ä¼çš„ç‰¢æˆ¿ï¼Œç¾åœ¨æ‡‰è©²æ˜¯å¤å ¡æœ€å—äººæ³¨ç›®çš„åœ°æ–¹ã€‚åœ°ä¸‹å®¤çš„å…¥å£ï¼Œåœ¨è¿‘åŸå ¡å…¥å£çš„ä¸€è™Ÿé™¢çš„å¾Œæ–¹ã€‚èµ°ä¸‹æ¢¯éšï¼Œç©¿éåœ°ä¸‹æ‹±å®¤ï¼Œä¾¿æ˜¯å¤å ¡çš„å›šç‰¢ã€‚ç•¶å¹´ä¸»å¼µæ—¥å…§ç“¦ç¨ç«‹çš„æ³¢å°¼ä¼ï¼Œä¾¿æ˜¯è¢«éµéˆé–åœ¨å›šç‰¢ä¸­çš„ç¬¬äº”æ ¹æŸ±å­ä¸Šã€‚æ—¢ç„¶æ˜¯ç‘å£«ç¬¬ä¸€å¤å ¡çš„åœ°ä¸‹å›šæˆ¿ï¼Œåˆæ˜¯æ‹œå€«è©©æ­Œä¸­æè¿°çš„åœ°æ–¹ï¼ŒæŸ±å­ä¸Šé›£å…è¢«åˆ»ä¸Šä¾†è‡ªä¸åŒåœ‹å®¶çš„éŠå®¢çš„ã€Œåˆ°æ­¤ä¸€éŠã€å­—å¥ã€‚ç•¢ç«Ÿï¼Œæ‹œå€«è‡ªå·±ä¹Ÿåœ¨ç¬¬ä¸‰æ ¹æŸ±å­ä¸Šåˆ»ä¸‹è‡ªå·±çš„åå­—ã€‚ä¸éï¼Œè²´äººåå£«çš„å¡—é´‰çµ‚ç©¶æ˜¯æœ‰é»ä¸åŒï¼Œç¾åœ¨æ‹œå€«çš„ç°½åå·²ç¶“è¢«ç»ç’ƒä¿è­·è‘—äº†ã€‚\næ‹œå€«çš„è©©ï¼Œè¬›è¿°æ³¢å°¼ä¼åŠä»–çš„å…©ä½å¼Ÿå¼Ÿè¢«é—œæŠ¼æ–¼æ­¤ã€‚ä»–å€‘è¢«é–åœ¨ä¸åŒçš„æŸ±å­ä¸Šï¼Œå¯ä»¥çœ‹è¦‹å°æ–¹ï¼Œå»è§¸æ‘¸ä¸åˆ°ã€‚æ³¢å°¼ä¼çš„å…©ä½å¼Ÿå¼Ÿæ²’èƒ½æ”¯æ’åˆ°æœ€å¾Œï¼Œéƒ½æ­»åœ¨ç‰¢ç„ä¹‹ä¸­ï¼Œåªå‰©ä¸‹æ³¢å°¼ä¼ä¸€äººã€‚æ‹œå€«çš„è©©ï¼Œå¯¦åœ¨æ˜¯ä¸€å€‹å¯æ­Œå¯æ³£çš„æ‚²æ…˜æ•…äº‹ã€‚å¯æ˜¯ï¼Œã€Œæ–‡äººå¤šå¤§è©±ã€é€™å¥è©±å¯æ˜¯æ²’æœ‰èªªéŒ¯ã€‚è©©ä¸­çš„ä¸»è§’æ³¢å°¼ä¼ï¼Œæ˜¯æ—¥å…§ç“¦è–ç¶­å…‹å¤šä¿®é“é™¢ (St. Victor) çš„é™¢é•·ï¼Œåå…­ä¸–ç´€åˆå› ä¸»å¼µåŠæ¨å‹•æ—¥å…§ç“¦ç¨ç«‹ï¼ŒåŠå°æŠ—è–©ä¼ä¾å…¬çˆµï¼Œè€Œè¢«å›šç¦æ–¼å¤å ¡ä¹‹ä¸­ã€‚è¢«å›šç¦çš„å››å¹´å¾Œï¼Œç‘å£«äººæˆåŠŸæ”»ä½”çŸ³åº¸åŸå ¡ï¼Œæ³¢å°¼ä¼çµ‚è¢«é‡‹æ”¾å‡ºä¾†ã€‚æ­·å²ä¸Šï¼Œæ³¢å°¼ä¼çš„å…©ä½å¼Ÿå¼Ÿå»æ˜¯ä¸å­˜åœ¨çš„ã€‚ç„¡è«–å¦‚ä½•ï¼Œæ‹œå€«å„ªç¾çš„æ–‡å­—å’Œå¯Œæ„ŸæŸ“åŠ›çš„æè¿°ï¼Œç¢ºå¯¦ç‚ºçŸ³åº¸å¤å ¡æ·»åŠ äº†ä¸å°‘å‚³å¥‡è‰²å½©ã€‚\nåœ¨åŸå ¡å…§ï¼Œå¹¾ä¹æ‰€æœ‰çš„æˆ¿é–“åŠè¨­æ–½éƒ½æ˜¯é–‹æ”¾çµ¦éŠäººåƒè§€çš„ã€‚å¯æ˜¯åœ¨è¿‘åŸé–€çš„ä¸€è™Ÿé™¢çš„ä¸€è§’ï¼Œåœ¨åå…­ä¸–ç´€æ™‚æ›¾ç¶“ç”¨ä½œé¦¬æ£šçš„åœ°æ–¹ï¼Œå·²è¢«æ”¹å»ºæˆæ–°å¼æˆ¿å±‹ï¼Œå°å†Šå­ä¸Šå¯«è‘—ç¾åœ¨æ˜¯å…¬å¯“ã€‚ç«Ÿç„¶æœ‰äººä½åœ¨ä¸€å€‹å°å¤–é–‹æ”¾çµ¦éŠäººåƒè§€çš„æ­·å²å¤è¹Ÿï¼Œå¯¦åœ¨æœ‰é»å¥‡æ€ªã€‚é™¤äº†æ›¾ç¶“å›šç¦æ³¢å°¼ä¼çš„ç›£ç„å¤–ï¼Œå…¶ä»–åœ°æ–¹å¦‚å ¡ä¸»å¤§å»³ã€å®´æœƒå»³ã€ç´‹ç« å¤§å»³ã€å…¬çˆµçš„å§å®¤ç­‰ç­‰ï¼Œéƒ½æ˜¯å¾ˆå€¼å¾—åƒè§€çš„åœ°æ–¹ã€‚éŠäººæ›´å¯èµ°ä¸ŠåŸå ¡çš„ä¸»å¡”ï¼Œä¿¯ç°åŸå ¡çš„å®å‰å»ºç¯‰æˆ–çœºæœ›å››å‘¨å£¯éº—çš„æ™¯è‰²ã€‚\nå¤å ¡åŠæ—¥å…§ç“¦æ¹–ç•”ï¼Œæ˜¥å¤ç§‹å†¬å„æœ‰ä¸åŒçš„æ™¯è‡´ï¼Œé›–ç„¶å¤šæ•¸éŠäººä¸æœƒæœ‰æ©Ÿæœƒè¦ªèº«é«”æœƒï¼Œåœ¨å¤å ¡å¤–çš„å°å•†åº—å…§å‡ºå”®çš„æ˜ä¿¡ç‰‡åŠæ›¸ç±ï¼Œå»ä¹Ÿå¯è®“éŠäººæ„Ÿå—ä¸€ä¸‹æ­¤åœ°å››å­£ä¸ åŒçš„æ°£æ°›ã€‚\nåƒè€ƒè³‡æ–™ ChÃ¢teau de chillon å®˜æ–¹ç¶²ç«™ ChÃ¢teau de chillon - Wikipedia ChÃ¢teau de chillon on Google Maps George Gordon Byron - Wikipedia FranÃ§ois Bonivard - Wikipedia The Prisoner of Chillon (Full Text) - Wikisource å»¶ä¼¸é–±è®€ ä½™ç§‹é›¨ï¼Œã€ˆå¸Œéš†çš„å›šå¾’ã€‰ï¼Œã€Šè¡Œè€…ç„¡ç–†ã€‹ï¼Œæ™‚å ±æ–‡åŒ–ï¼Œå°åŒ—ï¼ŒäºŒé›¶é›¶ä¸€å¹´ã€‚ æ¬’çŠç‘šï¼Œã€Šæ¼«æ­¥è•¾å¤¢æ¹–ã€‹ï¼Œå…ƒå°Šæ–‡åŒ–ï¼Œå°åŒ—ï¼Œä¸€ä¹ä¹å…«å¹´ã€‚ ","date":"2007-04-25T00:00:00Z","image":"https://albertauyeung.github.io/images/chillon00.jpg","permalink":"https://albertauyeung.github.io/2007/04/25/chateau-chillon.html","title":"ğŸ° ChÃ¢teau de Chillon ç‘å£«çŸ³åº¸å¤å ¡éŠè¨˜"}]